{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.598687660112\n",
      "[ 0.02402607  0.04805215  0.02402607]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    w = np.ones(len(x))\n",
    "    return  1 / (1 + np.exp(-np.dot(w, x)))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    w = np.ones(len(x))\n",
    "    e = np.exp(- np.dot(w, x))\n",
    "    return np.multiply(e / (1 + e)**2, x)\n",
    "\n",
    "print(sigmoid(np.array([0.1, 0.2, 0.1])))\n",
    "print(deriv_sigmoid(np.array([0.1, 0.2, 0.1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OR\n",
    "train_X = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
    "train_y = np.array([[1], [1], [0], [1]])\n",
    "test_X, test_y = train_X, train_y\n",
    "\n",
    "# weights\n",
    "W = np.random.uniform(low=-0.08, high=0.08, size=(2, 1)).astype('float32')\n",
    "b = np.zeros(1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 誤差関数\n",
    "* 負の対数尤度関数 (Negative Loglikelihood Function）\n",
    "* 交差エントロピー誤差関数ともいう\n",
    "\n",
    "$$ E ( {\\bf \\theta} ) =  -\\sum^N_{i=1} \\left[ t_i \\log y ({\\bf x}_i ; {\\bf \\theta}) + (1 - t_i) \\log \\{ 1 - y ({\\bf x}_i ; {\\bf \\theta}) \\}\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, t, eps=1.0):\n",
    "    global W, b # to access variables that defined outside of this function.\n",
    "    \n",
    "    # Forward Propagation\n",
    "    print(x, W)\n",
    "    y = 1/ (1 + np.exp(- np.matmul(x, W)))\n",
    "    \n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "    cost = np.sum(-t*np.log(y)-(1-t)*np.log(1-y))\n",
    "    # delta = np.sum(np.matmul(np.diag(y - t), x), axis=0)\n",
    "    delta = np.matmul(np.transpose(x), y - t)\n",
    "    \n",
    "    # Update Parameters\n",
    "    x_tr = np.transpose(x)\n",
    "    P = np.diag(np.multiply(y, 1 - y)).reshape(-1, len(x))\n",
    "    a = np.matmul(x_tr, P)\n",
    "    b = np.matmul(a, x)\n",
    "    Q = np.linalg.inv(b)\n",
    "    dW = np.matmul(np.matmul(Q, x_tr, y - t))\n",
    "    #db = # WRITE ME!\n",
    "    W = W - eps*dW\n",
    "    #b = b - eps*db\n",
    "\n",
    "    return cost\n",
    "\n",
    "def test(x, t):\n",
    "    # Test Cost\n",
    "    y = 1/ (1 + np.exp(- np.matmul(x, W)))\n",
    "    cost = np.sum(-t*np.log(y)-(1-t)*np.log(1-y))\n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def train(x, t, eps=1.0):\n",
    "    global W, b # to access variables that defined outside of this function.\n",
    "    \n",
    "    # Forward Propagation\n",
    "    y = sigmoid(np.matmul(x, W) + b)\n",
    "    \n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "    cost = np.sum(-t*np.log(y)-(1-t)*np.log(1-y))\n",
    "    delta = np.sum(y - t)\n",
    "    \n",
    "    # Update Parameters\n",
    "    dW = np.multiply(delta, x.reshape(2,1))\n",
    "    db = delta\n",
    "    W = W - eps*dW\n",
    "    b = b - eps*db\n",
    "\n",
    "    return cost\n",
    "\n",
    "def test(x, t):\n",
    "    # Test Cost\n",
    "    y = sigmoid(np.matmul(x, W) + b)\n",
    "    cost = np.sum(-t*np.log(y)-(1-t)*np.log(1-y))\n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test input [[0 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]]\n",
      "test output [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "predicted [[ 0.99933265]\n",
      " [ 0.99933176]\n",
      " [ 0.00166658]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Epoch\n",
    "for epoch in range(1000):\n",
    "    # Online Learning\n",
    "    for x, y in zip(train_X, train_y):\n",
    "        cost = train(x[np.newaxis, :], y[np.newaxis, :])\n",
    "    cost, pred_y = test(test_X, test_y)\n",
    "\n",
    "print('test input', test_X)\n",
    "print('test output', test_y)\n",
    "print('predicted', pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def deriv_softmax(x):\n",
    "    return softmax(x) * (1 - softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return # WRITE ME!\n",
    "\n",
    "\n",
    "def deriv_tanh(x):\n",
    "    return # WRITE ME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XOR\n",
    "train_X = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
    "train_y = np.array([[1], [1], [0], [0]])\n",
    "test_X, test_y = train_X, train_y\n",
    "\n",
    "# Layer1 weights\n",
    "W1 = np.random.uniform(low=-0.08, high=0.08, size=(2, 3)).astype('float32')\n",
    "b1 = np.zeros(3).astype('float32')\n",
    "\n",
    "# Layer2 weights\n",
    "W2 = np.random.uniform(low=-0.08, high=0.08, size=(3, 1)).astype('float32')\n",
    "b2 = np.zeros(1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 誤差関数\n",
    "* 負の対数尤度関数 (Negative Loglikelihood Function）\n",
    "* 交差エントロピー誤差関数ともいう\n",
    "\n",
    "$$ E ( {\\bf \\theta} ) =  -\\sum^N_{i=1} \\left[ t_i \\log y ({\\bf x}_i ; {\\bf \\theta}) + (1 - t_i) \\log \\{ 1 - y ({\\bf x}_i ; {\\bf \\theta}) \\}\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, t, eps=1.0):\n",
    "    global W1, b1, W2, b2 # to access variables that defined outside of this function.\n",
    "\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = sigmoid(u2)\n",
    "    \n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "    y = z2\n",
    "    cost = np.sum(-t*np.log(y) - (1 - t)*np.log(1 - y))\n",
    "    delta_2 = np.sum(y - t, axis=0)[np.newaxis, :]\n",
    "    delta_1 = deriv_sigmoid(u1).reshape(3, 1) * np.matmul(W2, delta_2)\n",
    "    \n",
    "    # Update Parameters Layer1\n",
    "    dW1 = np.matmul(delta_1, x).T\n",
    "    db1 = delta_1.reshape(3)\n",
    "    W1 = W1 - eps*dW1\n",
    "    b1 = b1 - eps*db1\n",
    "    \n",
    "    # Update Parameters Layer2\n",
    "    dW2 = np.matmul(delta_2, z1).T\n",
    "    db2 = delta_2.reshape(1)\n",
    "    W2 = W2 - eps*dW2\n",
    "    b2 = b2 - eps*db2\n",
    "\n",
    "    return cost\n",
    "\n",
    "def test(x, t):\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = sigmoid(u2)\n",
    "    \n",
    "    y = z2\n",
    "    \n",
    "    # Test Cost\n",
    "    cost = np.sum(-t*np.log(y)-(1-t)*np.log(1-y))\n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.98948747e-01]\n",
      " [  9.98910806e-01]\n",
      " [  8.12031650e-04]\n",
      " [  1.85537181e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Epoch\n",
    "for epoch in range(2000):\n",
    "    # Online Learning\n",
    "    for x, y in zip(train_X, train_y):\n",
    "        cost = train(x[np.newaxis, :], y[np.newaxis, :])\n",
    "    cost, pred_y = test(test_X, test_y)\n",
    "\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題. MNISTデータセットを多層パーセプトロン(MLP)で学習せよ\n",
    "\n",
    "### 注意\n",
    "- homework関数を完成させて提出してください\n",
    "    - 訓練データはtrain_X, train_y, テストデータはtest_Xで与えられます\n",
    "    - train_Xとtrain_yをtrain_X, train_yとvalid_X, valid_yに分けるなどしてモデルを学習させてください\n",
    "    - test_Xに対して予想ラベルpred_yを作り, homework関数の戻り値としてください\\\n",
    "- pred_yのtest_yに対する精度(F値)で評価します\n",
    "- 全体の実行時間がiLect上で60分を超えないようにしてください\n",
    "- homework関数の外には何も書かないでください (必要なものは全てhomework関数に入れてください)\n",
    "- 解答提出時には Answer Cell の内容のみを提出してください\n",
    "\n",
    "### ヒント\n",
    "- 出力yはone-of-k表現\n",
    "- 最終層の活性化関数はソフトマックス関数, 誤差関数は多クラス交差エントロピー\n",
    "- 最終層のデルタは教科書参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    def onehot(y):\n",
    "        # y: 0~9\n",
    "        v = np.zeros(10)\n",
    "        v[y] = 1.0\n",
    "        return v\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def deriv_sigmoid(x):\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    \n",
    "    def softmax(x):\n",
    "        exp = np.exp(x)\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    \n",
    "    LAYER0_UNITS = 784\n",
    "    LAYER1_UNITS = 100\n",
    "    LAYER2_UNITS = 10\n",
    "    EPS = 0.1\n",
    "    # Layer1 weights\n",
    "    W1 = np.random.uniform(low=-0.08, high=0.08, size=(LAYER0_UNITS, LAYER1_UNITS)).astype('float32')\n",
    "    b1 = np.zeros(LAYER1_UNITS).astype('float32')\n",
    "\n",
    "    # Layer2 weights\n",
    "    W2 = np.random.uniform(low=-0.08, high=0.08, size=(LAYER1_UNITS, LAYER2_UNITS)).astype('float32')\n",
    "    b2 = np.zeros(LAYER2_UNITS).astype('float32')\n",
    "    \n",
    "    for epoch in range(500):\n",
    "        # Online learning\n",
    "        for (X, t) in zip(train_X, train_y):\n",
    "            # X: (784), t: 0~9\n",
    "            _X, _t = X[np.newaxis, :], np.array([onehot(t)])\n",
    "            u1 = np.matmul(_X, W1) + b1\n",
    "            z1 = sigmoid(u1)\n",
    "            u2 = np.matmul(z1, W2) + b2\n",
    "            z2 = softmax(u2)\n",
    "            y = z2\n",
    "\n",
    "            # Cost and Delta for backpropagation\n",
    "            cost = - np.sum(np.matmul(np.log(y), _t.T))\n",
    "            delta_2 = np.sum(y - _t, axis=0)[np.newaxis, :]\n",
    "            delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T)\n",
    "\n",
    "            # Update Parameters Layer1\n",
    "            dW1 = np.matmul(_X.T, delta_1)\n",
    "            db1 = delta_1.reshape(LAYER1_UNITS)\n",
    "            W1 = W1 - EPS * dW1\n",
    "            b1 = b1 - EPS * db1\n",
    "            # Update Parameters Layer1\n",
    "            dW2 = np.matmul(z1.T, delta_2)\n",
    "            db2 = delta_2.reshape(LAYER2_UNITS)\n",
    "            W2 = W2 - EPS * dW2\n",
    "            b2 = b2 - EPS * db2\n",
    "\n",
    "    # Predict\n",
    "    u1 = np.matmul(test_X, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = softmax(u2)\n",
    "    pred_y = np.argmax(z2, axis=1)\n",
    "    \n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    mnist_X, mnist_y = shuffle(mnist.data.astype('float32'),\n",
    "                               mnist.target.astype('int32'), random_state=42)\n",
    "\n",
    "    mnist_X = mnist_X / 255.0\n",
    "\n",
    "    return train_test_split(mnist_X, mnist_y,\n",
    "                test_size=0.2,\n",
    "                random_state=42)\n",
    "\n",
    "def validate_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "\n",
    "    # validate for small dataset\n",
    "#     train_X_mini = train_X[:100]\n",
    "#     train_y_mini = train_y[:100]\n",
    "    train_X_mini = train_X[:20000]\n",
    "    train_y_mini = train_y[:20000]\n",
    "    test_X_mini = test_X[:100]\n",
    "    test_y_mini = test_y[:100]\n",
    "\n",
    "    pred_y = homework(train_X_mini, train_y_mini, test_X_mini)\n",
    "    print(test_y_mini[:20])\n",
    "    print(pred_y[:20])\n",
    "    print(\"f value\", f1_score(test_y_mini, pred_y, average='macro'))\n",
    "    print(\"accuracy\", accuracy(test_y_mini, pred_y))\n",
    "\n",
    "def score_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "    pred_y = homework(train_X, train_y, test_X)\n",
    "    print(f1_score(test_y, pred_y, average='macro'))\n",
    "    \n",
    "def accuracy(pred, label):\n",
    "    cmp = (pred == label)\n",
    "    return np.sum(cmp) / len(cmp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 5 8 9 0 6 6 3 9 5 8 2 0 6 9 0 8 0 1 3]\n",
      "[7 5 8 9 0 6 6 3 9 5 8 2 0 6 9 0 8 0 1 3]\n",
      "f value 0.980389016018\n",
      "accuracy 0.98\n"
     ]
    }
   ],
   "source": [
    "validate_homework()\n",
    "# score_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784) (56000,)\n",
      "(14000, 784) (14000,)\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape, train_y.shape)\n",
    "print(test_X.shape, test_y.shape)\n",
    "print(np.amax(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-e17a9a582d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore_homework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-133-24ef61ba6ba4>\u001b[0m in \u001b[0;36mscore_homework\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscore_homework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhomework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-5d9d3993a948>\u001b[0m in \u001b[0;36mhomework\u001b[0;34m(train_X, train_y, test_X)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mdW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mdb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLAYER1_UNITS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mEPS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdW1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mEPS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# Update Parameters Layer1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
