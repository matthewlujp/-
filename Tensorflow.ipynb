{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:: 10, cost:: 0.464\n",
      "step:: 20, cost:: 0.135\n",
      "step:: 30, cost:: 0.040\n",
      "step:: 40, cost:: 0.012\n",
      "step:: 50, cost:: 0.003\n",
      "step:: 60, cost:: 0.001\n",
      "step:: 70, cost:: 0.000\n",
      "step:: 80, cost:: 0.000\n",
      "step:: 90, cost:: 0.000\n",
      "step:: 100, cost:: 0.000\n",
      "pred_y: [ 13.00327778]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, name='x')\n",
    "t = tf.placeholder(tf.float32, name='t')\n",
    "\n",
    "w = tf.Variable(0.0, name='w')\n",
    "b = tf.Variable(0.0, name='b')\n",
    "\n",
    "y = w * x + b\n",
    "\n",
    "cost = tf.reduce_mean((y - t)**2)\n",
    "\n",
    "gw, gb = tf.gradients(cost, [w, b])\n",
    "updates = [\n",
    "    w.assign(w - 0.1 * gw),\n",
    "    b.assign(b - 0.1 * gb)\n",
    "]\n",
    "\n",
    "train = tf.group(*updates)\n",
    "\n",
    "data_X = np.array([0, 1, 2, 3, 4])\n",
    "data_y = np.array([3, 5, 7, 9, 11])\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(100):\n",
    "    _cost, _ = sess.run([cost, train], feed_dict={x: data_X, t: data_y})\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print('step:: %d, cost:: %.3f' % (i + 1, _cost))\n",
    "        \n",
    "print('pred_y:', sess.run(y, feed_dict={x:[5]}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    IMAGE_SIZE = 784\n",
    "    CATEGORY_NUM = 10\n",
    "    ETA = 0.1\n",
    "    EPSILON = 1e-5\n",
    "    EPOCHS = 200\n",
    "    BATCH_SIZE = 30\n",
    "    LAYER1_UNITS = 80\n",
    "    LAYER2_UNITS = 50\n",
    "    LAYER3_UNITS = CATEGORY_NUM\n",
    "\n",
    "    DROPOUT_LAYER1 = 0.5\n",
    "    DROPOUT_LAYER2 = 0.8\n",
    "\n",
    "    TRAIN_DATA_SIZE = len(train_X)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    with tf.variable_scope('Placeholders'):\n",
    "        images = tf.placeholder(tf.float32, [None, IMAGE_SIZE], name='image_data')\n",
    "        labels = tf.placeholder(tf.int64, None, name='label')\n",
    "        _labels = tf.one_hot(labels, depth=CATEGORY_NUM, on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "        apply_dropout = tf.placeholder(tf.bool, name='apply_dropout')\n",
    "\n",
    "    with tf.variable_scope('NetworkParams'):\n",
    "        W1 = tf.Variable(tf.truncated_normal([IMAGE_SIZE, LAYER1_UNITS]), name='W1')\n",
    "        # b1 = tf.Variable(tf.zeros(LAYER1_UNITS), name='b1')\n",
    "        W2 = tf.Variable(tf.truncated_normal([LAYER1_UNITS, LAYER2_UNITS]), name='W2')\n",
    "        # b2 = tf.Variable(tf.zeros(LAYER2_UNITS), name='b2')\n",
    "        W3 = tf.Variable(tf.truncated_normal([LAYER2_UNITS, LAYER3_UNITS]), name='W2')\n",
    "        # b3 = tf.Variable(tf.zeros(LAYER3_UNITS), name='b2')\n",
    "        params = [W1, W2, W3]\n",
    "\n",
    "    def dropout_apply(val, keep_prob, apply):\n",
    "        return tf.cond(apply, lambda: tf.nn.dropout(val, keep_prob), lambda: val)\n",
    "\n",
    "    def batch_normalization(X):\n",
    "        eps = EPSILON\n",
    "        output_dim = int(X.get_shape()[-1])\n",
    "        gamma = tf.Variable(tf.truncated_normal([output_dim], stddev=0.1), name='gamma')\n",
    "        beta = tf.Variable(tf.zeros([output_dim]), name='beta')\n",
    "        mean_X, var_X = tf.nn.moments(X, [0])\n",
    "        return gamma * (X - mean_X) / tf.sqrt(var_X + eps) + beta\n",
    "\n",
    "    u1 = batch_normalization(tf.matmul(images, W1))\n",
    "    z1 = tf.nn.relu(u1)\n",
    "    u2 = batch_normalization(tf.matmul(z1, W2))\n",
    "    z2 = tf.nn.relu(u2)\n",
    "    u3 = batch_normalization(tf.matmul(z2, W3))\n",
    "    y = u3\n",
    "    # y: [BATCH_SIZE x category_size]\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=_labels))\n",
    "    predicted = tf.argmax(y, axis=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), tf.float32))\n",
    "\n",
    "    # Update network params\n",
    "    # gW1, gb1, gW2, gb2, gW3, gb3 = tf.gradients(cost, params)\n",
    "    gW1, gW2, gW3 = tf.gradients(cost, params)\n",
    "    updates = [\n",
    "        W1.assign_add(- ETA * gW1),\n",
    "        # b1.assign_add(- ETA * gb1),\n",
    "        W2.assign_add(- ETA * gW2),\n",
    "        # b2.assign_add(- ETA * gb2),\n",
    "        W3.assign_add(- ETA * gW3),\n",
    "        # b3.assign_add(- ETA * gb3),\n",
    "    ]\n",
    "    train = tf.group(*updates)\n",
    "\n",
    "    # Training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "\n",
    "            # Train\n",
    "            step = 0\n",
    "            while step * BATCH_SIZE < TRAIN_DATA_SIZE:\n",
    "                start_idx = step * BATCH_SIZE\n",
    "                end_idx = start_idx + BATCH_SIZE\n",
    "                if not end_idx < TRAIN_DATA_SIZE:\n",
    "                    end_idx = TRAIN_DATA_SIZE\n",
    "\n",
    "                batch_X, batch_y = train_X[start_idx:end_idx, :], train_y[start_idx:end_idx]\n",
    "                _, c, a = sess.run([train, cost, accuracy],\n",
    "                                               feed_dict={images: batch_X, labels: batch_y,\n",
    "                                               apply_dropout: True})\n",
    "                epoch_loss += c\n",
    "                epoch_accuracy += a\n",
    "                step += 1\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                format_str = \"Epoch %d out of %d --- LOSS: %f --- ACCURACY: %f\"\n",
    "                print(format_str % (epoch, EPOCHS, epoch_loss, epoch_accuracy / step))\n",
    "\n",
    "        # Evaluation\n",
    "        pred_y = sess.run(predicted, feed_dict={images: test_X, apply_dropout: False})\n",
    "        \n",
    "        elapsed_time = (time.time() - start_time) / 60  # min\n",
    "        print(\"Elapsed time: %f min\" % elapsed_time)\n",
    "        \n",
    "        return pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 out of 500 --- LOSS: 386.349399 --- ACCURACY: 0.211178\n",
      "Epoch 100 out of 500 --- LOSS: 114.604620 --- ACCURACY: 0.903493\n",
      "Epoch 200 out of 500 --- LOSS: 102.879164 --- ACCURACY: 0.933733\n",
      "Epoch 300 out of 500 --- LOSS: 96.836139 --- ACCURACY: 0.950100\n",
      "Epoch 400 out of 500 --- LOSS: 92.898814 --- ACCURACY: 0.961876\n",
      "[1 5 8 9 0 6 6 3 9 5 8 2 0 4 9 0 8 0 1 3 2 0 2 5 7 7 1 1 9 0 2 7 7 1 3 3 2\n",
      " 5 8 7 9 8 6 8 6 2 4 6 3 4 7 5 1 0 7 2 1 0 0 4 5 3 7 2 1 1 5 8 6 4 4 0 9 8\n",
      " 4 4 0 9 8 2 9 2 3 5 7 4 0 2 6 2 2 9 7 1 2 7 7 2 3 0]\n",
      "0.904548107267\n"
     ]
    }
   ],
   "source": [
    "validate_homework()\n",
    "# score_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 out of 200 --- LOSS: 385.925522 --- ACCURACY: 0.095110\n",
      "Epoch 100 out of 200 --- LOSS: 358.551058 --- ACCURACY: 0.519461\n",
      "Elapsed time: 0.990315 min\n",
      "[1 2 9 9 8 6 6 1 9 8 8 2 6 6 9 7 9 0 1 6 2 0 2 8 7 2 1 1 9 0 0 7 7 1 9 8 2\n",
      " 8 9 7 9 9 6 7 6 2 6 6 7 4 7 5 1 8 7 4 1 2 0 9 8 8 7 2 1 1 8 9 6 8 9 6 9 9\n",
      " 6 9 6 9 8 2 9 2 7 8 7 9 0 2 6 2 2 6 7 1 2 7 7 2 8 6]\n",
      "0.495843674899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luning/.pyenv/versions/3.6.0/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "validate_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# del [\n",
    "#     tf.app,\n",
    "#     tf.compat,\n",
    "#     tf.contrib,\n",
    "#     tf.errors,\n",
    "#     tf.gfile,\n",
    "#     tf.graph_util,\n",
    "#     tf.image,\n",
    "#     tf.layers,\n",
    "#     tf.logging,\n",
    "#     tf.losses,\n",
    "#     tf.metrics,\n",
    "#     tf.python_io,\n",
    "#     tf.resource_loader,\n",
    "#     tf.saved_model,\n",
    "#     tf.sdca,\n",
    "#     tf.sets,\n",
    "#     tf.summary,\n",
    "#     tf.sysconfig,\n",
    "#     tf.test,\n",
    "#     tf.train\n",
    "# ]\n",
    "\n",
    "def load_mnist():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    mnist_X, mnist_y = shuffle(mnist.data.astype('float32'),\n",
    "                               mnist.target.astype('int32'), random_state=42)\n",
    "\n",
    "    mnist_X = mnist_X / 255.0\n",
    "\n",
    "    return train_test_split(mnist_X, mnist_y,\n",
    "                test_size=0.2,\n",
    "                random_state=42)\n",
    "\n",
    "def validate_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "\n",
    "    # validate for small dataset\n",
    "#     train_X_mini = train_X[:100]\n",
    "#     train_y_mini = train_y[:100]\n",
    "    train_X_mini = train_X[:2000]\n",
    "    train_y_mini = train_y[:2000]\n",
    "    test_X_mini = test_X[:100]\n",
    "    test_y_mini = test_y[:100]\n",
    "\n",
    "    pred_y = homework(train_X_mini, train_y_mini, test_X_mini)\n",
    "    print(test_y_mini)\n",
    "    print(pred_y)\n",
    "    print(f1_score(test_y_mini, pred_y, average='macro'))\n",
    "\n",
    "def score_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "    pred_y = homework(train_X, train_y, test_X)\n",
    "    print(f1_score(test_y, pred_y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 out of 500 --- LOSS: 400.118928 --- ACCURACY: 0.175050\n",
      "Epoch 100 out of 500 --- LOSS: 117.645703 --- ACCURACY: 0.892016\n",
      "Epoch 200 out of 500 --- LOSS: 105.000771 --- ACCURACY: 0.923653\n",
      "Epoch 300 out of 500 --- LOSS: 98.541256 --- ACCURACY: 0.945210\n",
      "Epoch 400 out of 500 --- LOSS: 94.329387 --- ACCURACY: 0.959880\n",
      "[1 5 8 9 0 6 6 3 9 5 3 2 0 6 9 0 8 0 1 3 2 0 2 3 7 2 1 1 9 0 0 7 7 1 3 3 2\n",
      " 5 8 7 9 8 6 7 6 2 4 6 3 4 7 5 1 0 9 4 1 2 0 4 5 3 7 2 1 1 5 8 6 4 9 0 9 8\n",
      " 4 4 0 9 8 2 9 2 3 5 7 4 0 2 6 2 2 7 7 1 2 7 7 2 3 0]\n",
      "0.905332252733\n"
     ]
    }
   ],
   "source": [
    "validate_homework()\n",
    "# score_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784) (56000,)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = load_mnist()\n",
    "print(train_X.shape, train_y.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.14117648,  0.49019608,  0.86666667,  0.99607843,\n",
       "        1.        ,  0.99607843,  0.60392159,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.78039217,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.60784316,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.79215688,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.93333334,\n",
       "        0.40392157,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.05882353,  0.33725491,  0.07450981,  0.07450981,  0.16470589,\n",
       "        0.58431375,  0.84705883,  0.99215686,  0.91764706,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.63921571,\n",
       "        0.99215686,  0.98431373,  0.39215687,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.19215687,  0.99215686,  0.99215686,\n",
       "        0.43137255,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.12941177,  0.99215686,  0.99215686,  0.43137255,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.29019609,  0.99215686,\n",
       "        0.99215686,  0.43137255,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.63921571,  0.99215686,  0.93333334,  0.09019608,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.25490198,  0.9137255 ,\n",
       "        0.99215686,  0.51764709,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.13725491,  0.90980393,  0.99215686,  0.91764706,  0.10588235,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.04313726,  0.61176473,  0.99215686,\n",
       "        0.85882354,  0.44705883,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.16078432,\n",
       "        0.35294119,  0.6901961 ,  0.28627452,  0.18039216,  0.18039216,\n",
       "        0.61176473,  0.99215686,  0.87450981,  0.29019609,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.06666667,  0.35294119,  0.95294118,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.43137255,  0.05490196,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.05882353,  0.74901962,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.69803923,\n",
       "        0.11764706,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.75294119,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.95294118,  0.57254905,\n",
       "        0.04705882,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.3137255 ,  0.97254902,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.93333334,\n",
       "        0.78431374,  0.40784314,  0.40784314,  0.49803922,  0.92941177,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.7647059 ,  0.3137255 ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.02745098,\n",
       "        0.97254902,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.97647059,  0.51764709,  0.09019608,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.06666667,  0.49803922,  0.97254902,\n",
       "        0.99215686,  0.99215686,  0.96862745,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.47843137,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.96862745,  0.38039216,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.33725491,  0.96862745,  0.99215686,\n",
       "        0.96862745,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.70980394,  0.99215686,  0.78039217,  0.48235294,\n",
       "        0.0627451 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.03921569,  0.48235294,  0.73333335,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    IMAGE_SIZE = 784\n",
    "    CATEGORY_NUM = 10\n",
    "    ETA = 0.15\n",
    "    EPSILON = 1e-5\n",
    "    DECAY = 0.5\n",
    "#     EPOCHS = 500\n",
    "    EPOCHS = 200\n",
    "    BATCH_SIZE = 30\n",
    "    LAYER1_UNITS = 100\n",
    "    LAYER2_UNITS = 100\n",
    "    LAYER3_UNITS = CATEGORY_NUM\n",
    "\n",
    "    TRAIN_DATA_SIZE = len(train_X)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # todo(matthew): Add operation to calculate data ave and var before test evaluation\n",
    "\n",
    "    with tf.variable_scope('Placeholders'):\n",
    "        images = tf.placeholder(tf.float32, [None, IMAGE_SIZE], name='image_data')\n",
    "        labels = tf.placeholder(tf.int64, None, name='label')\n",
    "        _labels = tf.one_hot(labels, depth=CATEGORY_NUM, on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "        is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "    with tf.variable_scope('NetworkParams'):\n",
    "        W1 = tf.Variable(tf.truncated_normal([IMAGE_SIZE, LAYER1_UNITS]), name='W1')\n",
    "        W2 = tf.Variable(tf.truncated_normal([LAYER1_UNITS, LAYER2_UNITS]), name='W2')\n",
    "        W3 = tf.Variable(tf.truncated_normal([LAYER2_UNITS, LAYER3_UNITS]), name='W3')\n",
    "        b3 = tf.Variable(tf.zeros(LAYER3_UNITS), name='b3')\n",
    "        # params = [W1, W2, W3, b3]\n",
    "\n",
    "    def batch_normalization(X, is_train):\n",
    "        eps = EPSILON\n",
    "        output_dim = int(X.shape[-1])\n",
    "\n",
    "        gamma = tf.Variable(tf.truncated_normal([output_dim], stddev=0.1, mean=1.0), name='gamma')\n",
    "        beta = tf.Variable(tf.zeros([output_dim]), name='beta')\n",
    "        mean_X, var_X = tf.nn.moments(X, [0])\n",
    "\n",
    "        pop_mean = tf.Variable(tf.zeros([output_dim]), trainable=False)\n",
    "        pop_var = tf.Variable(tf.ones([output_dim]), trainable=False)\n",
    "\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def sample():\n",
    "            ema_apply_op = ema.apply([mean_X, var_X])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                batch_size = tf.to_float(tf.shape(X)[0])\n",
    "                unbiased_var_X = batch_size / (batch_size - 1) * var_X\n",
    "                return tf.nn.batch_normalization(X, mean_X, var_X, beta, gamma, eps)\n",
    "            # Save exponential average\n",
    "            # pop_mean.assign(DECAY * pop_mean + (1 - DECAY) * mean_X)\n",
    "            # pop_var.assign(DECAY * pop_var + (1 - DECAY) * var_X)\n",
    "\n",
    "        def population():\n",
    "            train_mean = pop_mean.assign(ema.average(mean_X))\n",
    "            train_var = pop_mean.assign(ema.average(var_X))\n",
    "            return tf.nn.batch_normalization(X, train_mean, train_var, beta, gamma, eps)\n",
    "\n",
    "        return tf.cond(is_train, lambda: sample(), lambda: population())\n",
    "\n",
    "    u1 = batch_normalization(tf.matmul(images, W1), is_train)\n",
    "    z1 = tf.nn.relu(u1)\n",
    "    u2 = batch_normalization(tf.matmul(z1, W2), is_train)\n",
    "    z2 = tf.nn.relu(u2)\n",
    "    u3 = tf.matmul(z2, W3) + b3\n",
    "    y = u3\n",
    "    # y: [BATCH_SIZE x category_size]\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=_labels))\n",
    "    predicted = tf.argmax(y, axis=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), tf.float32))\n",
    "\n",
    "    # Update network params\n",
    "    params = tf.trainable_variables()\n",
    "    grads = tf.gradients(cost, params)\n",
    "    updates = [v.assign_add(- ETA * gv) for v, gv in zip(params, grads)]\n",
    "    # updates = [\n",
    "    #     W1.assign_add(- ETA * gW1),\n",
    "    #     W2.assign_add(- ETA * gW2),\n",
    "    #     W3.assign_add(- ETA * gW3),\n",
    "    #     b3.assign_add(- ETA * gb3),\n",
    "    # ]\n",
    "    train = tf.group(*updates)\n",
    "\n",
    "    # Training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "\n",
    "            # Train\n",
    "            step = 0\n",
    "            while step * BATCH_SIZE < TRAIN_DATA_SIZE:\n",
    "                start_idx = step * BATCH_SIZE\n",
    "                end_idx = start_idx + BATCH_SIZE\n",
    "                if not end_idx < TRAIN_DATA_SIZE:\n",
    "                    end_idx = TRAIN_DATA_SIZE\n",
    "\n",
    "                batch_X, batch_y = train_X[start_idx:end_idx, :], train_y[start_idx:end_idx]\n",
    "                _, c, a = sess.run([train, cost, accuracy],\n",
    "                                   feed_dict={images: batch_X, labels: batch_y,\n",
    "                                              is_train: True})\n",
    "                epoch_loss += c\n",
    "                epoch_accuracy += a\n",
    "                step += 1\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                format_str = \"Epoch %d out of %d --- LOSS: %f --- ACCURACY: %f\"\n",
    "                print(format_str % (epoch, EPOCHS, epoch_loss, epoch_accuracy / step))\n",
    "\n",
    "        # Evaluation\n",
    "        pred_y = sess.run(predicted, feed_dict={images: test_X, is_train: False})\n",
    "        elapsed_time = (time.time() - start_time) / 60  # min\n",
    "        print(\"Elapsed time: %f min\" % elapsed_time)\n",
    "        return pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 out of 200 --- LOSS: 318.482763 --- ACCURACY: 0.548303\n",
      "Epoch 100 out of 200 --- LOSS: 0.365093 --- ACCURACY: 1.000000\n",
      "Elapsed time: 1.688748 min\n",
      "[7 5 8 9 0 6 6 3 9 5 8 2 0 6 9 0 8 0 1 3 2 0 2 5 7 2 1 1 9 0 5 7 7 1 3 3 2\n",
      " 5 8 7 9 8 6 9 6 2 4 6 3 4 7 5 1 0 9 2 1 2 0 4 5 3 7 2 1 1 5 8 6 4 4 0 9 8\n",
      " 4 4 0 9 8 2 9 2 3 8 7 4 0 2 6 2 2 7 7 1 2 7 2 2 3 0]\n",
      "[9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n",
      "0.0181818181818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luning/.pyenv/versions/3.6.0/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Unbiased\n",
    "validate_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-44345d7883f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidate_homework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-950667387dfc>\u001b[0m in \u001b[0;36mvalidate_homework\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtest_y_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhomework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-4082b90727a1>\u001b[0m in \u001b[0;36mhomework\u001b[0;34m(train_X, train_y, test_X)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mu2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-4082b90727a1>\u001b[0m in \u001b[0;36mbatch_normalization\u001b[0;34m(X, is_train)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luning/.pyenv/versions/3.6.0/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mcond\u001b[0;34m(pred, fn1, fn2, name)\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0mcontext_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCondContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[0mcontext_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildCondBranch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m     \u001b[0mcontext_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExitResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m     \u001b[0mcontext_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luning/.pyenv/versions/3.6.0/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildCondBranch\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1640\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mBuildCondBranch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[0;34m\"\"\"Add the subgraph defined by fn() to the graph.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m     \u001b[0moriginal_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-4082b90727a1>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-4082b90727a1>\u001b[0m in \u001b[0;36mpopulation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mtrain_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mtrain_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_X' is not defined"
     ]
    }
   ],
   "source": [
    "validate_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 out of 500 --- LOSS: 247.373713 --- ACCURACY: 0.168000\n",
      "Epoch 100 out of 500 --- LOSS: 66.540016 --- ACCURACY: 0.928667\n",
      "Epoch 200 out of 500 --- LOSS: 57.991087 --- ACCURACY: 0.965333\n",
      "Epoch 300 out of 500 --- LOSS: 53.752415 --- ACCURACY: 0.983333\n",
      "Epoch 400 out of 500 --- LOSS: 51.137289 --- ACCURACY: 0.989667\n",
      "Elapsed time: 2.209841 min\n",
      "[1 5 1 1 0 6 6 3 1 5 1 5 0 6 4 0 1 0 1 3 2 0 2 1 7 7 1 1 1 0 0 7 7 1 5 3 2\n",
      " 5 4 7 9 1 6 7 6 1 4 6 5 4 7 5 1 0 5 5 1 2 0 4 5 1 7 2 1 1 1 6 6 4 1 0 1 1\n",
      " 4 4 0 9 1 4 9 2 3 5 1 9 0 1 6 2 1 7 1 1 2 7 7 2 3 0]\n",
      "0.622444144091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luning/.pyenv/versions/3.6.0/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "validate_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 out of 300 --- LOSS: 444.877840 --- ACCURACY: 0.524751\n",
      "Epoch 100 out of 300 --- LOSS: 1.008190 --- ACCURACY: 1.000000\n",
      "Epoch 200 out of 300 --- LOSS: 0.407033 --- ACCURACY: 1.000000\n",
      "Elapsed time: 1.880607 min\n",
      "[7 5 8 9 0 6 6 3 9 5 8 2 0 6 9 0 8 0 1 3 2 0 2 5 7 2 1 1 9 0 5 7 7 1 3 3 2\n",
      " 5 8 7 9 8 6 9 6 2 4 6 3 4 7 5 1 0 9 2 1 2 0 4 5 3 7 2 1 1 5 8 6 4 4 0 9 8\n",
      " 4 4 0 9 8 2 9 2 3 8 7 4 0 2 6 2 2 7 7 1 2 7 2 2 3 0]\n",
      "[2 8 2 9 0 6 8 3 9 5 8 2 0 6 9 0 8 0 2 2 2 0 2 8 7 2 2 2 9 0 1 2 7 2 3 3 2\n",
      " 5 2 7 9 8 6 2 6 2 7 6 8 8 7 5 2 0 7 2 2 2 0 9 5 3 7 2 2 2 8 8 6 4 2 0 8 2\n",
      " 8 4 0 9 8 2 9 2 3 8 7 9 0 2 6 2 2 7 7 2 2 7 2 2 2 0]\n",
      "0.649983997696\n"
     ]
    }
   ],
   "source": [
    "validate_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    IMAGE_SIZE = 784\n",
    "    CATEGORY_NUM = 10\n",
    "    ETA = 0.15\n",
    "    EPSILON = 1e-5\n",
    "    DECAY = 0.99\n",
    "    EPOCHS = 600\n",
    "#     EPOCHS = 201\n",
    "    BATCH_SIZE = 30\n",
    "    LAYER1_UNITS = 100\n",
    "    LAYER2_UNITS = 100\n",
    "    LAYER3_UNITS = CATEGORY_NUM\n",
    "\n",
    "    TRAIN_DATA_SIZE = len(train_X)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # todo(matthew): Add operation to calculate data ave and var before test evaluation\n",
    "\n",
    "    with tf.variable_scope('Placeholders'):\n",
    "        images = tf.placeholder(tf.float32, [None, IMAGE_SIZE], name='image_data')\n",
    "        labels = tf.placeholder(tf.int64, None, name='label')\n",
    "        _labels = tf.one_hot(labels, depth=CATEGORY_NUM, on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "        is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "    with tf.variable_scope('NetworkParams'):\n",
    "        W1 = tf.Variable(tf.truncated_normal([IMAGE_SIZE, LAYER1_UNITS]), name='W1')\n",
    "        W2 = tf.Variable(tf.truncated_normal([LAYER1_UNITS, LAYER2_UNITS]), name='W2')\n",
    "        W3 = tf.Variable(tf.truncated_normal([LAYER2_UNITS, LAYER3_UNITS]), name='W3')\n",
    "        b3 = tf.Variable(tf.zeros(LAYER3_UNITS), name='b3')\n",
    "        # params = [W1, W2, W3, b3]\n",
    "\n",
    "    def batch_normalization(X, is_train):\n",
    "        eps = EPSILON\n",
    "        output_dim = int(X.shape[-1])\n",
    "\n",
    "        gamma = tf.Variable(tf.truncated_normal([output_dim], stddev=0.1, mean=1.0), name='gamma')\n",
    "        beta = tf.Variable(tf.zeros([output_dim]), name='beta')\n",
    "\n",
    "        pop_mean = tf.Variable(tf.zeros([output_dim]), trainable=False)\n",
    "        pop_var = tf.Variable(tf.ones([output_dim]), trainable=False)\n",
    "\n",
    "        def sample():\n",
    "            mean_X, var_X = tf.nn.moments(X, [0])\n",
    "            # Save exponential average\n",
    "            moving_avg_op = tf.group(\n",
    "                pop_mean.assign(DECAY * pop_mean + (1 - DECAY) * mean_X),\n",
    "                pop_var.assign(DECAY * pop_var + (1 - DECAY) * var_X),\n",
    "            )\n",
    "\n",
    "            with tf.control_dependencies([moving_avg_op]):\n",
    "                batch_size = tf.to_float(tf.shape(X)[0])\n",
    "                unbiased_var_X = batch_size / (batch_size - 1) * var_X\n",
    "                return tf.nn.batch_normalization(X, mean_X, var_X, beta, gamma, eps)\n",
    "\n",
    "        def population():\n",
    "            return tf.nn.batch_normalization(X, pop_mean, pop_var, beta, gamma, eps)\n",
    "\n",
    "        return tf.cond(is_train, lambda: sample(), lambda: population())\n",
    "\n",
    "    def adam_optimizer():\n",
    "        alpha = 0.001\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        eps = 1e-8\n",
    "\n",
    "        delta = 1.0\n",
    "\n",
    "        return delta\n",
    "\n",
    "    u1 = batch_normalization(tf.matmul(images, W1), is_train)\n",
    "    z1 = tf.nn.relu(u1)\n",
    "    u2 = batch_normalization(tf.matmul(z1, W2), is_train)\n",
    "    z2 = tf.nn.relu(u2)\n",
    "    u3 = tf.matmul(z2, W3) + b3\n",
    "    y = u3\n",
    "    # y: [BATCH_SIZE x category_size]\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=_labels))\n",
    "    predicted = tf.argmax(y, axis=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), tf.float32))\n",
    "\n",
    "    # Update network params\n",
    "    params = tf.trainable_variables()\n",
    "    grads = tf.gradients(cost, params)\n",
    "    updates = [v.assign_add(- ETA * gv) for v, gv in zip(params, grads)]\n",
    "    train = tf.group(*updates)\n",
    "\n",
    "    # Training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "\n",
    "            # Train\n",
    "            step = 0\n",
    "            while step * BATCH_SIZE < TRAIN_DATA_SIZE:\n",
    "                start_idx = step * BATCH_SIZE\n",
    "                end_idx = start_idx + BATCH_SIZE\n",
    "                if not end_idx < TRAIN_DATA_SIZE:\n",
    "                    end_idx = TRAIN_DATA_SIZE\n",
    "\n",
    "                batch_X, batch_y = train_X[start_idx:end_idx, :], train_y[start_idx:end_idx]\n",
    "                _, c, a = sess.run([train, cost, accuracy],\n",
    "                                   feed_dict={images: batch_X, labels: batch_y,\n",
    "                                              is_train: True})\n",
    "                epoch_loss += c\n",
    "                epoch_accuracy += a\n",
    "                step += 1\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                format_str = \"Epoch %d out of %d --- LOSS: %f --- ACCURACY: %f\"\n",
    "                print(format_str % (epoch, EPOCHS, epoch_loss, epoch_accuracy / step))\n",
    "\n",
    "        # Evaluation\n",
    "        pred_y = sess.run(predicted, feed_dict={images: test_X, is_train: False})\n",
    "        elapsed_time = (time.time() - start_time) / 60  # min\n",
    "        print(\"Elapsed time: %f min\" % elapsed_time)\n",
    "        return pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 out of 201 --- LOSS: 331.460501 --- ACCURACY: 0.545110\n",
      "Epoch 100 out of 201 --- LOSS: 0.426021 --- ACCURACY: 1.000000\n",
      "Epoch 200 out of 201 --- LOSS: 0.137046 --- ACCURACY: 1.000000\n",
      "Elapsed time: 1.545775 min\n",
      "[7 5 8 9 0 6 6 3 9 5 8 2 0 6 9 0 8 0 1 3 2 0 2 5 7 2 1 1 9 0 5 7 7 1 3 3 2\n",
      " 5 8 7 9 8 6 9 6 2 4 6 3 4 7 5 1 0 9 2 1 2 0 4 5 3 7 2 1 1 5 8 6 4 4 0 9 8\n",
      " 4 4 0 9 8 2 9 2 3 8 7 4 0 2 6 2 2 7 7 1 2 7 2 2 3 0]\n",
      "[1 5 8 9 0 6 6 3 9 5 3 2 0 6 9 0 8 0 1 3 2 0 2 5 7 2 1 1 9 0 3 7 7 1 7 3 2\n",
      " 5 8 7 9 8 6 7 6 2 4 6 8 4 7 5 1 0 7 2 1 2 0 4 3 3 7 2 1 1 5 4 6 4 9 0 9 8\n",
      " 4 4 9 9 8 2 9 2 3 8 7 4 0 2 6 2 2 7 7 1 2 7 7 2 3 0]\n",
      "0.873514175173\n"
     ]
    }
   ],
   "source": [
    "validate_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "\n",
    "def _var_key(var):\n",
    "    return (var.op.graph, var.op.name)\n",
    "\n",
    "\n",
    "def _valid_dtypes():\n",
    "    # Valid types for loss, variables, and gradients\n",
    "    return set([tf.float16, tf.float32, tf.float64])\n",
    "\n",
    "\n",
    "def _assert_valid_dtypes(self, tensors):\n",
    "    valid_dtypes = self._valid_dtypes()\n",
    "    for t in tensors:\n",
    "        dtype = t.dtype.base_dtype\n",
    "        if dtype not in valid_dtypes:\n",
    "            raise ValueError(\"Invalid type %r for %s, expected: %s\"\n",
    "                             % (dtype, t.name, [v for v in valid_dtypes]))\n",
    "\n",
    "\n",
    "class EveOptimizer:\n",
    "    def __init__(self, learning_rate=.001, beta1=0.1, beta2=0.999, beta3=0.999, epsilon=1e-8,\n",
    "                 k=.1, K=10, name=\"Eve\"):\n",
    "        self._lr = learning_rate\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._beta3 = beta3\n",
    "        self._epsilon = epsilon\n",
    "        self._k = k\n",
    "        self._K = K\n",
    "\n",
    "        self._delte = None\n",
    "        self._Delte = None\n",
    "        self._beta1_power = None\n",
    "        self._beta2_power = None\n",
    "        # self._f_1_hat = None\n",
    "        self._f_2_hat = None\n",
    "        self._step = None\n",
    "\n",
    "        # {slot1: {var1: xxx, var2: xxx, ...}, slot2: {var1: xxx, var2: xxx, ...}, ...}\n",
    "        self._slots = {}\n",
    "\n",
    "    def _get_beta_accumulators(self):\n",
    "        return self._beta1_power, self._beta2_power\n",
    "\n",
    "    def _accumulate_beta(self):\n",
    "        op1 = tf.assign(self._beta1_power, self._beta1_power * self._beta1)\n",
    "        op2 = tf.assign(self._beta1_power, self._beta1_power * self._beta1)\n",
    "        return tf.groupt([op1, op2])\n",
    "\n",
    "    def _create_algo_params(self, var_list):\n",
    "        if (self._beta1_power is None or self._beta1_power.graph is not var_list[0].graph):\n",
    "            with ops.colocate_with(var_list[0]):\n",
    "                self._beta1_power = tf.Variable(0., name='beta1_power',\n",
    "                                                            trainable=False)\n",
    "                self._beta2_power = tf.Variable(0., name='beta2_power',\n",
    "                                                            trainable=False)\n",
    "                self._delta = tf.Variable(self._k + 1., name='delta', trainable=False)\n",
    "                self._Delta = tf.Variable(self._K + 1., name='Delta', trainable=False)\n",
    "                # self._f_1_hat = variable_scope.variable(0, name='f_1_hat', trainable=False)\n",
    "                self._f_2_hat = tf.Variable(0., name='f_2_hat', trainable=False)\n",
    "                self._step = tf.Variable(0, name='step', trainable=False)\n",
    "\n",
    "            for v in var_list:\n",
    "                self._create_slot(v, 0., \"m\")\n",
    "                self._create_slot(v, 0., \"v\")\n",
    "                self._create_slot(v, 1., \"d\")\n",
    "\n",
    "    def _create_slot(self, var, initializer, slot_name):\n",
    "        if slot_name not in self._slots.keys():\n",
    "            self._slots[slot_name] = {}\n",
    "        named_slot = self._slots[slot_name]\n",
    "\n",
    "        with tf.variable_scope(slot_name):\n",
    "            self.named_slot[_var_key(var)] = tf.Variable(initializer, name=var.op.name, trainable=False)\n",
    "\n",
    "    def _get_slot(self, var, slot_name):\n",
    "        named_slot = self._slots[slot_name]\n",
    "        return named_slot[_var_key(var)]\n",
    "\n",
    "    def _compute_gradients(self, cost, var_list=None):\n",
    "        _assert_valid_dtypes([loss])\n",
    "        if var_list is None:\n",
    "            var_list = tf.trainable_variables()\n",
    "        else:\n",
    "            nest.flatten(var_list)\n",
    "        var_list += tf.get_collection(tf.GraphKeys._STREAMING_MODEL_PORTS)\n",
    "        grads = tf.gradients(cost, var_list)\n",
    "        grads_and_vars = list(zip(grads, var_list))\n",
    "        return grads_and_vars\n",
    "\n",
    "    def minimize(self, cost, var_list=None):\n",
    "        self._create_algo_params(var_list)\n",
    "\n",
    "        # [(g1, v1), (g2, v2), ...]\n",
    "        grads_and_vars = self._compute_gradients(cost, var_list)\n",
    "\n",
    "        beta_acc_op = self._accumulate_beta()\n",
    "        updates = []\n",
    "\n",
    "        with tf.control_dependencies([bata_acc_op]):\n",
    "            beta1_pow, beta2_pow = self._get_beta_accumulators()\n",
    "            for g, var in grads_and_vars:\n",
    "                m = self._get_slot(v, \"m\")\n",
    "                op_mt = tf.assign(m, self._beta1 * m + (1 - self._beta1) * g)\n",
    "                with tf.control_dependencies([op_mt]):\n",
    "                    m_t_hat = m / (1 - beta1_pow)\n",
    "\n",
    "                v = self._get_slot(v, \"v\")\n",
    "                op_vt = tf.assign(v, self._beta2 * v + (1 - self._beta2) * g * g)\n",
    "                with tf.control_dependencies([op_vt]):\n",
    "                    v_t_hat = v / (1 - beta2_pow)\n",
    "\n",
    "                d = self._get_slot(v, \"d\")\n",
    "\n",
    "                def first_step():\n",
    "                    op1 = tf.assign(self._f_2_hat, cost)\n",
    "                    op2 = tf.assign(d, 1.)\n",
    "                    return tf.group([op1, op2])\n",
    "\n",
    "                def other_step():\n",
    "                    delte, Delte = tf.cond(cost > self._f_2_hat, lambda: self.k + 1, self.K + 1,\n",
    "                                           lambda: 1. / (self.K + 1), 1. / (self.k + 1))\n",
    "                    c_t = tf.minimum(tf.maximum(delte, cost / self._f_2_hat), Delte)\n",
    "                    f_1_hat = c_t * self._f_2_hat\n",
    "                    r_t = tf.abs(f_1_hat - self._f_2_hat) / tf.minimum(f_1_hat, self._f_2_hat)\n",
    "\n",
    "                    op1 = tf.assign(d, self._beta3 * d + (1 - self._beta3) * r_t)\n",
    "                    op2 = tf.assign(self._f_2_hat, f_1_hat)\n",
    "                    return tf.group([op1, op2])\n",
    "\n",
    "                update_algo_params = tf.cond(self._step > 0, other_step, first_step)\n",
    "                with control_dependencies([update_algo_params]):\n",
    "                    update_param = var.assign_add(\n",
    "                        - self._lr * m_t_hat / (d * tf.sqrt(v_t_hat) + self._epsilon))\n",
    "\n",
    "                updates.append(update_param)\n",
    "                updates.append(tf.assign(m, m_t))\n",
    "                updates.append(tf.assign(v, v_t))\n",
    "\n",
    "            update_step = self._step.assign_add(1)\n",
    "            updates.append(update_step)\n",
    "            return tf.group(updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# del [\n",
    "#     tf.app,\n",
    "#     tf.compat,\n",
    "#     tf.contrib,\n",
    "#     tf.errors,\n",
    "#     tf.gfile,\n",
    "#     tf.graph_util,\n",
    "#     tf.image,\n",
    "#     tf.layers,\n",
    "#     tf.logging,\n",
    "#     tf.losses,\n",
    "#     tf.metrics,\n",
    "#     tf.python_io,\n",
    "#     tf.resource_loader,\n",
    "#     tf.saved_model,\n",
    "#     tf.sdca,\n",
    "#     tf.sets,\n",
    "#     tf.summary,\n",
    "#     tf.sysconfig,\n",
    "#     tf.test,\n",
    "#     tf.train\n",
    "# ]\n",
    "\n",
    "def load_mnist():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    mnist_X, mnist_y = shuffle(mnist.data.astype('float32'),\n",
    "                               mnist.target.astype('int32'), random_state=42)\n",
    "\n",
    "    mnist_X = mnist_X / 255.0\n",
    "\n",
    "    return train_test_split(mnist_X, mnist_y,\n",
    "                test_size=0.2,\n",
    "                random_state=42)\n",
    "\n",
    "def validate_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "\n",
    "    # validate for small dataset\n",
    "#     train_X_mini = train_X[:100]\n",
    "#     train_y_mini = train_y[:100]\n",
    "    train_X_mini = train_X[:2000]\n",
    "    train_y_mini = train_y[:2000]\n",
    "    test_X_mini = test_X[:100]\n",
    "    test_y_mini = test_y[:100]\n",
    "\n",
    "    pred_y = homework(train_X_mini, train_y_mini, test_X_mini)\n",
    "    print(test_y_mini)\n",
    "    print(pred_y)\n",
    "    print(f1_score(test_y_mini, pred_y, average='macro'))\n",
    "\n",
    "def score_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "    pred_y = homework(train_X, train_y, test_X)\n",
    "    print(f1_score(test_y, pred_y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    IMAGE_SIZE = 784\n",
    "    CATEGORY_NUM = 10\n",
    "    ETA = 0.15\n",
    "    EPSILON = 1e-5\n",
    "    DECAY = 0.99\n",
    "    EPOCHS = 600\n",
    "#     EPOCHS = 201\n",
    "    BATCH_SIZE = 30\n",
    "    LAYER1_UNITS = 100\n",
    "    LAYER2_UNITS = 100\n",
    "    LAYER3_UNITS = CATEGORY_NUM\n",
    "\n",
    "    TRAIN_DATA_SIZE = len(train_X)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # todo(matthew): Add operation to calculate data ave and var before test evaluation\n",
    "\n",
    "    with tf.variable_scope('Placeholders'):\n",
    "        images = tf.placeholder(tf.float32, [None, IMAGE_SIZE], name='image_data')\n",
    "        labels = tf.placeholder(tf.int64, None, name='label')\n",
    "        _labels = tf.one_hot(labels, depth=CATEGORY_NUM, on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "        is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "    with tf.variable_scope('NetworkParams'):\n",
    "        W1 = tf.Variable(tf.truncated_normal([IMAGE_SIZE, LAYER1_UNITS]), name='W1')\n",
    "        W2 = tf.Variable(tf.truncated_normal([LAYER1_UNITS, LAYER2_UNITS]), name='W2')\n",
    "        W3 = tf.Variable(tf.truncated_normal([LAYER2_UNITS, LAYER3_UNITS]), name='W3')\n",
    "        b3 = tf.Variable(tf.zeros(LAYER3_UNITS), name='b3')\n",
    "        # params = [W1, W2, W3, b3]\n",
    "\n",
    "    def batch_normalization(X, is_train):\n",
    "        eps = EPSILON\n",
    "        output_dim = int(X.shape[-1])\n",
    "\n",
    "        gamma = tf.Variable(tf.truncated_normal([output_dim], stddev=0.1, mean=1.0), name='gamma')\n",
    "        beta = tf.Variable(tf.zeros([output_dim]), name='beta')\n",
    "\n",
    "        pop_mean = tf.Variable(tf.zeros([output_dim]), trainable=False)\n",
    "        pop_var = tf.Variable(tf.ones([output_dim]), trainable=False)\n",
    "\n",
    "        def sample():\n",
    "            mean_X, var_X = tf.nn.moments(X, [0])\n",
    "            # Save exponential average\n",
    "            moving_avg_op = tf.group(\n",
    "                pop_mean.assign(DECAY * pop_mean + (1 - DECAY) * mean_X),\n",
    "                pop_var.assign(DECAY * pop_var + (1 - DECAY) * var_X),\n",
    "            )\n",
    "\n",
    "            with tf.control_dependencies([moving_avg_op]):\n",
    "                batch_size = tf.to_float(tf.shape(X)[0])\n",
    "                unbiased_var_X = batch_size / (batch_size - 1) * var_X\n",
    "                return tf.nn.batch_normalization(X, mean_X, var_X, beta, gamma, eps)\n",
    "\n",
    "        def population():\n",
    "            return tf.nn.batch_normalization(X, pop_mean, pop_var, beta, gamma, eps)\n",
    "\n",
    "        return tf.cond(is_train, lambda: sample(), lambda: population())\n",
    "\n",
    "\n",
    "    u1 = batch_normalization(tf.matmul(images, W1), is_train)\n",
    "    z1 = tf.nn.relu(u1)\n",
    "    u2 = batch_normalization(tf.matmul(z1, W2), is_train)\n",
    "    z2 = tf.nn.relu(u2)\n",
    "    u3 = tf.matmul(z2, W3) + b3\n",
    "    y = u3\n",
    "    # y: [BATCH_SIZE x category_size]\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=_labels))\n",
    "    predicted = tf.argmax(y, axis=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), tf.float32))\n",
    "\n",
    "    # Update network params\n",
    "    params = tf.trainable_variables()\n",
    "#     grads = tf.gradients(cost, params)\n",
    "#     updates = [v.assign_add(- ETA * gv) for v, gv in zip(params, grads)]\n",
    "#     train = tf.group(*updates)\n",
    "    train = EveOptimizer().minimize(cost, params)\n",
    "\n",
    "    # Training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "\n",
    "            # Train\n",
    "            step = 0\n",
    "            while step * BATCH_SIZE < TRAIN_DATA_SIZE:\n",
    "                start_idx = step * BATCH_SIZE\n",
    "                end_idx = start_idx + BATCH_SIZE\n",
    "                if not end_idx < TRAIN_DATA_SIZE:\n",
    "                    end_idx = TRAIN_DATA_SIZE\n",
    "\n",
    "                batch_X, batch_y = train_X[start_idx:end_idx, :], train_y[start_idx:end_idx]\n",
    "                _, c, a = sess.run([train, cost, accuracy],\n",
    "                                   feed_dict={images: batch_X, labels: batch_y,\n",
    "                                              is_train: True})\n",
    "                epoch_loss += c\n",
    "                epoch_accuracy += a\n",
    "                step += 1\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                format_str = \"Epoch %d out of %d --- LOSS: %f --- ACCURACY: %f\"\n",
    "                print(format_str % (epoch, EPOCHS, epoch_loss, epoch_accuracy / step))\n",
    "\n",
    "        # Evaluation\n",
    "        pred_y = sess.run(predicted, feed_dict={images: test_X, is_train: False})\n",
    "        elapsed_time = (time.time() - start_time) / 60  # min\n",
    "        print(\"Elapsed time: %f min\" % elapsed_time)\n",
    "        return pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EveOptimizer' object has no attribute 'named_slot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-44345d7883f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidate_homework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-950667387dfc>\u001b[0m in \u001b[0;36mvalidate_homework\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtest_y_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhomework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f0a2a98fb7ae>\u001b[0m in \u001b[0;36mhomework\u001b[0;34m(train_X, train_y, test_X)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m#     updates = [v.assign_add(- ETA * gv) for v, gv in zip(params, grads)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#     train = tf.group(*updates)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEveOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-64c3fd4af35c>\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, cost, var_list)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_algo_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# [(g1, v1), (g2, v2), ...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-64c3fd4af35c>\u001b[0m in \u001b[0;36m_create_algo_params\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-64c3fd4af35c>\u001b[0m in \u001b[0;36m_create_slot\u001b[0;34m(self, var, initializer, slot_name)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_slot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslot_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EveOptimizer' object has no attribute 'named_slot'"
     ]
    }
   ],
   "source": [
    "validate_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# del [\n",
    "#     tf.app,\n",
    "#     tf.compat,\n",
    "#     tf.contrib,\n",
    "#     tf.errors,\n",
    "#     tf.gfile,\n",
    "#     tf.graph_util,\n",
    "#     tf.image,\n",
    "#     tf.layers,\n",
    "#     tf.logging,\n",
    "#     tf.losses,\n",
    "#     tf.metrics,\n",
    "#     tf.python_io,\n",
    "#     tf.resource_loader,\n",
    "#     tf.saved_model,\n",
    "#     tf.sdca,\n",
    "#     tf.sets,\n",
    "#     tf.summary,\n",
    "#     tf.sysconfig,\n",
    "#     tf.test,\n",
    "#     tf.train\n",
    "# ]\n",
    "\n",
    "def load_mnist():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    mnist_X, mnist_y = shuffle(mnist.data.astype('float32'),\n",
    "                               mnist.target.astype('int32'), random_state=42)\n",
    "\n",
    "    mnist_X = mnist_X / 255.0\n",
    "\n",
    "    return train_test_split(mnist_X, mnist_y,\n",
    "                test_size=0.2,\n",
    "                random_state=42)\n",
    "\n",
    "def validate_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "\n",
    "    # validate for small dataset\n",
    "#     train_X_mini = train_X[:100]\n",
    "#     train_y_mini = train_y[:100]\n",
    "    train_X_mini = train_X[:5000]\n",
    "    train_y_mini = train_y[:5000]\n",
    "    test_X_mini = test_X[:100]\n",
    "    test_y_mini = test_y[:100]\n",
    "\n",
    "    pred_y = homework(train_X_mini, train_y_mini, test_X_mini)\n",
    "    print(test_y_mini)\n",
    "    print(pred_y)\n",
    "    print(f1_score(test_y_mini, pred_y, average='macro'))\n",
    "\n",
    "def score_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "    pred_y = homework(train_X, train_y, test_X)\n",
    "    print(f1_score(test_y, pred_y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    IMAGE_SIZE = 784\n",
    "    CATEGORY_NUM = 10\n",
    "    ETA = 0.15\n",
    "    EPSILON = 1e-5\n",
    "    DECAY = 0.99\n",
    "\n",
    "    LAYER1_UNITS = 100\n",
    "    LAYER2_UNITS = 100\n",
    "    LAYER3_UNITS = CATEGORY_NUM\n",
    "\n",
    "    TRAIN_DATA_SIZE = len(train_X)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # todo(matthew): Add operation to calculate data ave and var before test evaluation\n",
    "\n",
    "    with tf.variable_scope('Placeholders'):\n",
    "        images = tf.placeholder(tf.float32, [None, IMAGE_SIZE], name='image_data')\n",
    "        labels = tf.placeholder(tf.int64, None, name='label')\n",
    "        _labels = tf.one_hot(labels, depth=CATEGORY_NUM, on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "        is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "    with tf.variable_scope('NetworkParams'):\n",
    "        W1 = tf.Variable(tf.truncated_normal([IMAGE_SIZE, LAYER1_UNITS]), name='W1')\n",
    "        W2 = tf.Variable(tf.truncated_normal([LAYER1_UNITS, LAYER2_UNITS]), name='W2')\n",
    "        W3 = tf.Variable(tf.truncated_normal([LAYER2_UNITS, LAYER3_UNITS]), name='W3')\n",
    "        b3 = tf.Variable(tf.zeros(LAYER3_UNITS), name='b3')\n",
    "        # params = [W1, W2, W3, b3]\n",
    "\n",
    "    def batch_normalization(X, is_train):\n",
    "        eps = EPSILON\n",
    "        output_dim = int(X.shape[-1])\n",
    "\n",
    "        gamma = tf.Variable(tf.truncated_normal([output_dim], stddev=0.1, mean=1.0), name='gamma')\n",
    "        beta = tf.Variable(tf.zeros([output_dim]), name='beta')\n",
    "\n",
    "        pop_mean = tf.Variable(tf.zeros([output_dim]), trainable=False)\n",
    "        pop_var = tf.Variable(tf.ones([output_dim]), trainable=False)\n",
    "\n",
    "        def sample():\n",
    "            mean_X, var_X = tf.nn.moments(X, [0])\n",
    "            # Save exponential average\n",
    "            moving_avg_op = tf.group(\n",
    "                pop_mean.assign(DECAY * pop_mean + (1 - DECAY) * mean_X),\n",
    "                pop_var.assign(DECAY * pop_var + (1 - DECAY) * var_X),\n",
    "            )\n",
    "\n",
    "            with tf.control_dependencies([moving_avg_op]):\n",
    "                batch_size = tf.to_float(tf.shape(X)[0])\n",
    "                unbiased_var_X = batch_size / (batch_size - 1) * var_X\n",
    "                return tf.nn.batch_normalization(X, mean_X, var_X, beta, gamma, eps)\n",
    "\n",
    "        def population():\n",
    "            return tf.nn.batch_normalization(X, pop_mean, pop_var, beta, gamma, eps)\n",
    "\n",
    "        return tf.cond(is_train, lambda: sample(), lambda: population())\n",
    "\n",
    "\n",
    "    u1 = batch_normalization(tf.matmul(images, W1), is_train)\n",
    "    z1 = tf.nn.relu(u1)\n",
    "    u2 = batch_normalization(tf.matmul(z1, W2), is_train)\n",
    "    z2 = tf.nn.relu(u2)\n",
    "    u3 = tf.matmul(z2, W3) + b3\n",
    "    y = u3\n",
    "    # y: [BATCH_SIZE x category_size]\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=_labels))\n",
    "    predicted = tf.argmax(y, axis=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), tf.float32))\n",
    "\n",
    "    # Update network params\n",
    "    params = tf.trainable_variables()\n",
    "#     grads = tf.gradients(cost, params)\n",
    "#     updates = [v.assign_add(- ETA * gv) for v, gv in zip(params, grads)]\n",
    "#     train = tf.group(*updates)\n",
    "    train = EveOptimizer().minimize(cost, params)\n",
    "#     train = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    # Prepare data\n",
    "    t_X, v_X, t_y, v_y = train_test_split(train_X, train_y)\n",
    "    \n",
    "    # Training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        train_summary_writer = tf.summary.FileWriter(\"./optimizer/train\", sess.graph)\n",
    "\n",
    "        EPOCHS = 200\n",
    "        BATCH_SIZE = 30\n",
    "        STEPS_IN_EPOCHS = len(t_X) // BATCH_SIZE\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Train\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            for step in range(STEPS_IN_EPOCHS):\n",
    "                start_idx = step * BATCH_SIZE\n",
    "                end_idx = start_idx + BATCH_SIZE\n",
    "\n",
    "                batch_X, batch_y = train_X[start_idx:end_idx], train_y[start_idx:end_idx]\n",
    "                _, c, a = sess.run([train, cost, accuracy],\n",
    "                                   feed_dict={images: t_X[start_idx:end_idx],\n",
    "                                              labels: t_y[start_idx:end_idx],\n",
    "                                              is_train: True})\n",
    "                epoch_loss += c\n",
    "                epoch_accuracy += a\n",
    "\n",
    "            c, p = sess.run([cost, predicted],\n",
    "                            feed_dict={images: v_X, labels: v_y, is_train: False})\n",
    "            if epoch % 10 == 0:\n",
    "                format_str = \"Epoch %d   --- Train loss: %f   ---Train accuracy: %f   ---Test loss: %f   --- F: %f\"\n",
    "                print(format_str % (epoch, epoch_loss, epoch_accuracy / step, c, f1_score(v_y, p, average='macro')))\n",
    "                \n",
    "        train_summary_writer.close()\n",
    "\n",
    "        # Evaluation\n",
    "        pred_y = sess.run(predicted, feed_dict={images: test_X, is_train: False})\n",
    "        elapsed_time = (time.time() - start_time) / 60  # min\n",
    "        print(\"Elapsed time: %f min\" % elapsed_time)\n",
    "        return pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "\n",
    "def _var_key(var):\n",
    "    return (var.op.graph, var.op.name)\n",
    "\n",
    "\n",
    "def _valid_dtypes():\n",
    "    # Valid types for loss, variables, and gradients\n",
    "    return set([tf.float16, tf.float32, tf.float64])\n",
    "\n",
    "\n",
    "def _assert_valid_dtypes(tensors):\n",
    "    valid_dtypes = _valid_dtypes()\n",
    "    for t in tensors:\n",
    "        dtype = t.dtype.base_dtype\n",
    "        if dtype not in valid_dtypes:\n",
    "            raise ValueError(\"Invalid type %r for %s, expected: %s\"\n",
    "                             % (dtype, t.name, [v for v in valid_dtypes]))\n",
    "\n",
    "\n",
    "class EveOptimizer:\n",
    "    def __init__(self, learning_rate=.001, beta1=0.1, beta2=0.999, beta3=0.999, epsilon=1e-8,\n",
    "                 k=.1, K=10, name=\"Eve\"):\n",
    "        self._lr = learning_rate\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._beta3 = beta3\n",
    "        self._epsilon = epsilon\n",
    "        self._k = k\n",
    "        self._K = K\n",
    "\n",
    "        self._delte = None\n",
    "        self._Delte = None\n",
    "        self._beta1_power = None\n",
    "        self._beta2_power = None\n",
    "        # self._f_1_hat = None\n",
    "        self._f_2_hat = None\n",
    "        self._step = None\n",
    "\n",
    "        # {slot1: {var1: xxx, var2: xxx, ...}, slot2: {var1: xxx, var2: xxx, ...}, ...}\n",
    "        self._slots = {}\n",
    "\n",
    "    def _get_beta_accumulators(self):\n",
    "        return self._beta1_power, self._beta2_power\n",
    "\n",
    "    def _accumulate_beta(self):\n",
    "        op1 = tf.assign(self._beta1_power, self._beta1_power * self._beta1)\n",
    "        op2 = tf.assign(self._beta1_power, self._beta1_power * self._beta1)\n",
    "        return tf.group(op1, op2)\n",
    "\n",
    "    def _create_algo_params(self, var_list):\n",
    "        if (self._beta1_power is None or self._beta1_power.graph is not var_list[0].graph):\n",
    "            with ops.colocate_with(var_list[0]):\n",
    "                self._beta1_power = tf.Variable(0., name='beta1_power',\n",
    "                                                trainable=False)\n",
    "                self._beta2_power = tf.Variable(0., name='beta2_power',\n",
    "                                                trainable=False)\n",
    "                self._delta = tf.Variable(self._k + 1., name='delta', trainable=False)\n",
    "                self._Delta = tf.Variable(self._K + 1., name='Delta', trainable=False)\n",
    "                # self._f_1_hat = variable_scope.variable(0, name='f_1_hat', trainable=False)\n",
    "                self._f_2_hat = tf.Variable(0., name='f_2_hat', trainable=False)\n",
    "                self._step = tf.Variable(0, name='step', trainable=False)\n",
    "\n",
    "            for v in var_list:\n",
    "                self._create_slot(v, tf.zeros(v.shape), \"m\")\n",
    "                self._create_slot(v, tf.zeros(v.shape), \"v\")\n",
    "                self._create_slot(v, tf.ones(v.shape), \"d\")\n",
    "\n",
    "    def _create_slot(self, var, initializer, slot_name):\n",
    "        if slot_name not in self._slots.keys():\n",
    "            self._slots[slot_name] = {}\n",
    "        named_slot = self._slots[slot_name]\n",
    "\n",
    "        with tf.variable_scope(slot_name):\n",
    "            named_slot[_var_key(var)] = tf.Variable(initializer, name=var.op.name, trainable=False)\n",
    "\n",
    "    def _get_slot(self, var, slot_name):\n",
    "        named_slot = self._slots[slot_name]\n",
    "        return named_slot[_var_key(var)]\n",
    "\n",
    "    def _compute_gradients(self, cost, var_list=None):\n",
    "        _assert_valid_dtypes([cost])\n",
    "        if var_list is None:\n",
    "            var_list = tf.trainable_variables()\n",
    "        else:\n",
    "            nest.flatten(var_list)\n",
    "#         var_list += tf.get_collection(tf.GraphKeys._STREAMING_MODEL_PORTS)\n",
    "        grads = tf.gradients(cost, var_list)\n",
    "        grads_and_vars = list(zip(grads, var_list))\n",
    "        return grads_and_vars\n",
    "\n",
    "    def minimize(self, cost, var_list=None):\n",
    "        self._create_algo_params(var_list)\n",
    "\n",
    "        # [(g1, v1), (g2, v2), ...]\n",
    "        grads_and_vars = self._compute_gradients(cost, var_list)\n",
    "\n",
    "        beta_acc_op = self._accumulate_beta()\n",
    "        updates = []\n",
    "\n",
    "        with tf.control_dependencies([beta_acc_op]):\n",
    "            beta1_pow, beta2_pow = self._get_beta_accumulators()\n",
    "            for g, var in grads_and_vars:\n",
    "                m = self._get_slot(var, \"m\")\n",
    "                op_mt = tf.assign(m, self._beta1 * m + (1 - self._beta1) * g)\n",
    "                with tf.control_dependencies([op_mt]):\n",
    "                    m_t_hat = m / (1 - beta1_pow)\n",
    "\n",
    "                v = self._get_slot(var, \"v\")\n",
    "                op_vt = tf.assign(v, self._beta2 * v + (1 - self._beta2) * g * g)\n",
    "                with tf.control_dependencies([op_vt]):\n",
    "                    v_t_hat = v / (1 - beta2_pow)\n",
    "\n",
    "                d = self._get_slot(var, \"d\")\n",
    "\n",
    "                def first_step():\n",
    "                    op1 = tf.assign(self._f_2_hat, cost)\n",
    "                    op2 = tf.assign(d, tf.ones(d.shape))\n",
    "                    return tf.group(op1, op2)\n",
    "\n",
    "                def other_step():\n",
    "                    delte, Delte = tf.cond(\n",
    "                        cost > self._f_2_hat,\n",
    "                        lambda: (self._k + tf.constant(1.), self._K + tf.constant(1.)),\n",
    "                        lambda: (tf.constant(1.) / (self._K + tf.constant(1.)), tf.constant(1.) / (self._k + tf.constant(1.))))\n",
    "                    c_t = tf.minimum(tf.maximum(delte, cost / self._f_2_hat), Delte)\n",
    "                    f_1_hat = c_t * self._f_2_hat\n",
    "                    r_t = tf.abs(f_1_hat - self._f_2_hat) / tf.minimum(f_1_hat, self._f_2_hat)\n",
    "\n",
    "                    op1 = tf.assign(d, self._beta3 * d + (1 - self._beta3) * r_t)\n",
    "                    op2 = tf.assign(self._f_2_hat, f_1_hat)\n",
    "                    return tf.group(op1, op2)\n",
    "\n",
    "                update_algo_params = tf.cond(\n",
    "                    self._step > tf.constant(0, dtype=tf.int32),\n",
    "                    other_step, first_step)\n",
    "                with tf.control_dependencies([update_algo_params]):\n",
    "                    update_param = var.assign_add(\n",
    "                        - self._lr * m_t_hat / (d * tf.sqrt(v_t_hat) + self._epsilon))\n",
    "\n",
    "                updates.append(update_param)\n",
    "\n",
    "            update_step = self._step.assign_add(1)\n",
    "            updates.append(update_step)\n",
    "            return tf.group(*updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   --- Train loss: 182.643246   ---Train accuracy: 0.694086   ---Test loss: 1.102165   --- F: 0.804541\n",
      "Epoch 10   --- Train loss: 4.772926   ---Train accuracy: 1.005645   ---Test loss: 0.541614   --- F: 0.873027\n",
      "Epoch 20   --- Train loss: 1.073665   ---Train accuracy: 1.008065   ---Test loss: 0.550403   --- F: 0.882928\n",
      "Epoch 30   --- Train loss: 0.390026   ---Train accuracy: 1.008065   ---Test loss: 0.564469   --- F: 0.888620\n",
      "Epoch 40   --- Train loss: 0.161135   ---Train accuracy: 1.008065   ---Test loss: 0.580563   --- F: 0.886828\n",
      "Epoch 50   --- Train loss: 0.071664   ---Train accuracy: 1.008065   ---Test loss: 0.596380   --- F: 0.890871\n",
      "Epoch 60   --- Train loss: 0.033050   ---Train accuracy: 1.008065   ---Test loss: 0.615038   --- F: 0.896486\n",
      "Epoch 70   --- Train loss: 0.015921   ---Train accuracy: 1.008065   ---Test loss: 0.635010   --- F: 0.899031\n",
      "Epoch 80   --- Train loss: 0.007977   ---Train accuracy: 1.008065   ---Test loss: 0.655579   --- F: 0.899711\n",
      "Epoch 90   --- Train loss: 0.003793   ---Train accuracy: 1.008065   ---Test loss: 0.677844   --- F: 0.902156\n",
      "Epoch 100   --- Train loss: 0.001806   ---Train accuracy: 1.008065   ---Test loss: 0.698737   --- F: 0.906816\n",
      "Epoch 110   --- Train loss: 0.000897   ---Train accuracy: 1.008065   ---Test loss: 0.719109   --- F: 0.907589\n",
      "Epoch 120   --- Train loss: 0.000466   ---Train accuracy: 1.008065   ---Test loss: 0.739710   --- F: 0.906813\n",
      "Epoch 130   --- Train loss: 0.000243   ---Train accuracy: 1.008065   ---Test loss: 0.760826   --- F: 0.906813\n",
      "Epoch 140   --- Train loss: 0.000124   ---Train accuracy: 1.008065   ---Test loss: 0.782536   --- F: 0.908409\n",
      "Epoch 150   --- Train loss: 0.000065   ---Train accuracy: 1.008065   ---Test loss: 0.803722   --- F: 0.908458\n",
      "Epoch 160   --- Train loss: 0.000033   ---Train accuracy: 1.008065   ---Test loss: 0.825766   --- F: 0.909189\n",
      "Epoch 170   --- Train loss: 0.000017   ---Train accuracy: 1.008065   ---Test loss: 0.846006   --- F: 0.908270\n",
      "Epoch 180   --- Train loss: 0.000009   ---Train accuracy: 1.008065   ---Test loss: 0.865133   --- F: 0.909039\n",
      "Epoch 190   --- Train loss: 0.000005   ---Train accuracy: 1.008065   ---Test loss: 0.881983   --- F: 0.907399\n",
      "Elapsed time: 3.358480 min\n",
      "[7 5 8 9 0 6 6 3 9 5 8 2 0 6 9 0 8 0 1 3 2 0 2 5 7 2 1 1 9 0 5 7 7 1 3 3 2\n",
      " 5 8 7 9 8 6 9 6 2 4 6 3 4 7 5 1 0 9 2 1 2 0 4 5 3 7 2 1 1 5 8 6 4 4 0 9 8\n",
      " 4 4 0 9 8 2 9 2 3 8 7 4 0 2 6 2 2 7 7 1 2 7 2 2 3 0]\n",
      "[2 5 8 9 0 6 6 3 9 5 3 2 0 4 9 0 8 0 1 3 2 0 2 5 7 2 1 1 9 0 7 3 7 1 3 3 2\n",
      " 5 8 7 9 8 6 7 6 2 4 6 3 4 7 5 1 0 9 2 1 2 0 4 5 3 7 2 1 1 5 9 6 4 9 0 9 8\n",
      " 4 4 0 9 8 2 9 2 3 6 7 4 0 2 6 2 2 3 7 1 2 7 7 2 3 0]\n",
      "0.88510306515\n"
     ]
    }
   ],
   "source": [
    "# With Eve\n",
    "validate_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   --- Train loss: 697.326056   ---Train accuracy: 0.252688   ---Test loss: 4.859257   --- F: 0.389220\n",
      "Epoch 10   --- Train loss: 29.610290   ---Train accuracy: 0.948387   ---Test loss: 0.715975   --- F: 0.827468\n",
      "Epoch 20   --- Train loss: 6.256111   ---Train accuracy: 1.003763   ---Test loss: 0.621250   --- F: 0.856757\n",
      "Epoch 30   --- Train loss: 1.585718   ---Train accuracy: 1.008065   ---Test loss: 0.612522   --- F: 0.867729\n",
      "Epoch 40   --- Train loss: 0.612943   ---Train accuracy: 1.008065   ---Test loss: 0.618257   --- F: 0.870161\n",
      "Epoch 50   --- Train loss: 0.270264   ---Train accuracy: 1.008065   ---Test loss: 0.631642   --- F: 0.875146\n",
      "Epoch 60   --- Train loss: 0.125036   ---Train accuracy: 1.008065   ---Test loss: 0.648230   --- F: 0.879929\n",
      "Epoch 70   --- Train loss: 0.059453   ---Train accuracy: 1.008065   ---Test loss: 0.667984   --- F: 0.883164\n",
      "Epoch 80   --- Train loss: 0.028579   ---Train accuracy: 1.008065   ---Test loss: 0.689095   --- F: 0.886307\n",
      "Epoch 90   --- Train loss: 0.013879   ---Train accuracy: 1.008065   ---Test loss: 0.711465   --- F: 0.889402\n",
      "Epoch 100   --- Train loss: 0.006795   ---Train accuracy: 1.008065   ---Test loss: 0.734706   --- F: 0.890974\n",
      "Epoch 110   --- Train loss: 0.003343   ---Train accuracy: 1.008065   ---Test loss: 0.759013   --- F: 0.890260\n",
      "Epoch 120   --- Train loss: 0.001655   ---Train accuracy: 1.008065   ---Test loss: 0.784046   --- F: 0.890989\n",
      "Epoch 130   --- Train loss: 0.000824   ---Train accuracy: 1.008065   ---Test loss: 0.809770   --- F: 0.890227\n",
      "Epoch 140   --- Train loss: 0.000411   ---Train accuracy: 1.008065   ---Test loss: 0.836163   --- F: 0.893386\n",
      "Epoch 150   --- Train loss: 0.000206   ---Train accuracy: 1.008065   ---Test loss: 0.862273   --- F: 0.891933\n",
      "Epoch 160   --- Train loss: 0.000104   ---Train accuracy: 1.008065   ---Test loss: 0.888299   --- F: 0.891933\n",
      "Epoch 170   --- Train loss: 0.000053   ---Train accuracy: 1.008065   ---Test loss: 0.914453   --- F: 0.893558\n",
      "Epoch 180   --- Train loss: 0.000027   ---Train accuracy: 1.008065   ---Test loss: 0.940484   --- F: 0.895091\n",
      "Epoch 190   --- Train loss: 0.000014   ---Train accuracy: 1.008065   ---Test loss: 0.965885   --- F: 0.894982\n",
      "Elapsed time: 1.585944 min\n",
      "[7 5 8 9 0 6 6 3 9 5 8 2 0 6 9 0 8 0 1 3 2 0 2 5 7 2 1 1 9 0 5 7 7 1 3 3 2\n",
      " 5 8 7 9 8 6 9 6 2 4 6 3 4 7 5 1 0 9 2 1 2 0 4 5 3 7 2 1 1 5 8 6 4 4 0 9 8\n",
      " 4 4 0 9 8 2 9 2 3 8 7 4 0 2 6 2 2 7 7 1 2 7 2 2 3 0]\n",
      "[8 5 8 9 0 6 6 3 9 5 3 2 0 6 9 0 8 0 1 3 2 0 2 5 7 2 1 1 9 0 3 7 7 1 3 3 2\n",
      " 5 8 7 9 8 6 8 6 2 2 6 3 4 7 5 1 0 9 2 1 2 0 4 5 3 7 2 1 1 5 9 6 4 9 0 5 8\n",
      " 4 4 0 9 8 2 9 2 3 8 7 4 0 2 6 2 2 6 7 1 2 7 7 2 3 0]\n",
      "0.893830532213\n"
     ]
    }
   ],
   "source": [
    "# With Adam\n",
    "validate_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# del [\n",
    "#     tf.app,\n",
    "#     tf.compat,\n",
    "#     tf.contrib,\n",
    "#     tf.errors,\n",
    "#     tf.gfile,\n",
    "#     tf.graph_util,\n",
    "#     tf.image,\n",
    "#     tf.layers,\n",
    "#     tf.logging,\n",
    "#     tf.losses,\n",
    "#     tf.metrics,\n",
    "#     tf.python_io,\n",
    "#     tf.resource_loader,\n",
    "#     tf.saved_model,\n",
    "#     tf.sdca,\n",
    "#     tf.sets,\n",
    "#     tf.summary,\n",
    "#     tf.sysconfig,\n",
    "#     tf.test,\n",
    "#     tf.train\n",
    "# ]\n",
    "\n",
    "def load_mnist():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    mnist_X, mnist_y = shuffle(mnist.data.astype('float32'),\n",
    "                               mnist.target.astype('int32'), random_state=42)\n",
    "\n",
    "    mnist_X = mnist_X / 255.0\n",
    "\n",
    "    return train_test_split(mnist_X, mnist_y,\n",
    "                test_size=0.2,\n",
    "                random_state=42)\n",
    "\n",
    "def validate_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "\n",
    "    # validate for small dataset\n",
    "#     train_X_mini = train_X[:100]\n",
    "#     train_y_mini = train_y[:100]\n",
    "    train_X_mini = train_X[:5000]\n",
    "    train_y_mini = train_y[:5000]\n",
    "    test_X_mini = test_X[:100]\n",
    "    test_y_mini = test_y[:100]\n",
    "\n",
    "    pred_y = homework(train_X_mini, train_y_mini, test_X_mini)\n",
    "    print(test_y_mini)\n",
    "    print(pred_y)\n",
    "    print(f1_score(test_y_mini, pred_y, average='macro'))\n",
    "\n",
    "def score_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "    pred_y = homework(train_X, train_y, test_X)\n",
    "    print(f1_score(test_y, pred_y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "\n",
    "def _var_key(var):\n",
    "    return (var.op.graph, var.op.name)\n",
    "\n",
    "\n",
    "def _valid_dtypes():\n",
    "    # Valid types for loss, variables, and gradients\n",
    "    return set([tf.float16, tf.float32, tf.float64])\n",
    "\n",
    "\n",
    "def _assert_valid_dtypes(tensors):\n",
    "    valid_dtypes = _valid_dtypes()\n",
    "    for t in tensors:\n",
    "        dtype = t.dtype.base_dtype\n",
    "        if dtype not in valid_dtypes:\n",
    "            raise ValueError(\"Invalid type %r for %s, expected: %s\"\n",
    "                             % (dtype, t.name, [v for v in valid_dtypes]))\n",
    "\n",
    "\n",
    "class EveOptimizer:\n",
    "    def __init__(self, learning_rate=.001, beta1=0.9, beta2=0.999, beta3=0.999, epsilon=1e-8,\n",
    "                 k=.1, K=10, name=\"Eve\"):\n",
    "        self._lr = learning_rate\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._beta3 = beta3\n",
    "        self._epsilon = epsilon\n",
    "        self._k = k\n",
    "        self._K = K\n",
    "\n",
    "        self._delte = None\n",
    "        self._Delte = None\n",
    "        self._beta1_power = None\n",
    "        self._beta2_power = None\n",
    "        # self._f_1_hat = None\n",
    "        self._f_2_hat = None\n",
    "        self._step = None\n",
    "\n",
    "        # {slot1: {var1: xxx, var2: xxx, ...}, slot2: {var1: xxx, var2: xxx, ...}, ...}\n",
    "        self._slots = {}\n",
    "\n",
    "    def _get_beta_accumulators(self):\n",
    "        return self._beta1_power, self._beta2_power\n",
    "\n",
    "    def _accumulate_beta(self):\n",
    "        op1 = tf.assign(self._beta1_power, self._beta1_power * self._beta1)\n",
    "        op2 = tf.assign(self._beta2_power, self._beta2_power * self._beta2)\n",
    "        return tf.group(op1, op2)\n",
    "\n",
    "    def _create_algo_params(self, var_list):\n",
    "        if (self._beta1_power is None or self._beta1_power.graph is not var_list[0].graph):\n",
    "            with ops.colocate_with(var_list[0]):\n",
    "                self._beta1_power = tf.Variable(1., name='beta1_power',\n",
    "                                                trainable=False)\n",
    "                self._beta2_power = tf.Variable(1., name='beta2_power',\n",
    "                                                trainable=False)\n",
    "                self._f_2_hat = tf.Variable(0., name='f_2_hat', trainable=False)\n",
    "                self._step = tf.Variable(1, name='step', trainable=False)\n",
    "\n",
    "            for v in var_list:\n",
    "                self._create_slot(v, tf.zeros(v.shape), \"m\")\n",
    "                self._create_slot(v, tf.zeros(v.shape), \"v\")\n",
    "                self._create_slot(v, tf.ones(v.shape), \"d\")\n",
    "\n",
    "    def _create_slot(self, var, initializer, slot_name):\n",
    "        if slot_name not in self._slots.keys():\n",
    "            self._slots[slot_name] = {}\n",
    "        named_slot = self._slots[slot_name]\n",
    "\n",
    "        with tf.variable_scope(slot_name):\n",
    "            named_slot[_var_key(var)] = tf.Variable(initializer, name=var.op.name, trainable=False)\n",
    "\n",
    "    def _get_slot(self, var, slot_name):\n",
    "        named_slot = self._slots[slot_name]\n",
    "        return named_slot[_var_key(var)]\n",
    "\n",
    "    def _compute_gradients(self, cost, var_list=None):\n",
    "        _assert_valid_dtypes([cost])\n",
    "        if var_list is None:\n",
    "            var_list = tf.trainable_variables()\n",
    "        else:\n",
    "            nest.flatten(var_list)\n",
    "        grads = tf.gradients(cost, var_list)\n",
    "        grads_and_vars = list(zip(grads, var_list))\n",
    "        return grads_and_vars\n",
    "\n",
    "    def minimize(self, cost, var_list=None):\n",
    "        self._create_algo_params(var_list)\n",
    "\n",
    "        # [(g1, v1), (g2, v2), ...]\n",
    "        grads_and_vars = self._compute_gradients(cost, var_list)\n",
    "\n",
    "        beta_acc_op = self._accumulate_beta()\n",
    "        updates = []\n",
    "\n",
    "        # u1 = self._get_slot(var_list[1], \"m\")\n",
    "        # u2 = self._get_slot(var_list[2], \"m\")\n",
    "        # u3 = self._get_slot(var_list[3], \"m\")\n",
    "        # pr_out = tf.Print(u1, [u1, u2, u3], \"outside\")\n",
    "        # updates.append(pr_out)\n",
    "\n",
    "        with tf.control_dependencies([beta_acc_op]):\n",
    "            beta1_pow, beta2_pow = self._get_beta_accumulators()\n",
    "            # updates.append(tf.assert_less_equal(self._step, 7))\n",
    "            for g, var in grads_and_vars:\n",
    "                m = self._get_slot(var, \"m\")\n",
    "\n",
    "                # pr_before = tf.Print(m, [self._step, beta1_pow, var.op.name, m, self._beta1 * m + (1 - self._beta1) * g], \"m before update: \")\n",
    "                # updates.append(pr_before)\n",
    "\n",
    "                op_mt = tf.assign(m, self._beta1 * m + (1 - self._beta1) * g)\n",
    "                with tf.control_dependencies([op_mt]):\n",
    "                    updates.append(tf.assert_greater(1 - beta1_pow, 0.))\n",
    "                    updates.append(tf.Print(beta1_pow, [self._step, beta1_pow], \"after beta1_pow\"))\n",
    "                    m_t_hat = m / (1 - beta1_pow)\n",
    "                    # pr_m = tf.Print(m_t_hat, [self._step, beta1_pow, var.op.name, m, m_t_hat], \"m after update: \")\n",
    "                    # updates.append(pr_m)\n",
    "\n",
    "                v = self._get_slot(var, \"v\")\n",
    "                op_vt = tf.assign(v, self._beta2 * v + (1 - self._beta2) * g * g)\n",
    "                with tf.control_dependencies([op_vt]):\n",
    "                    updates.append(tf.assert_greater(1 - beta2_pow, 0.))\n",
    "                    updates.append(tf.Print(beta2_pow, [self._step, beta2_pow], \"after beta2_pow\"))\n",
    "                    v_t_hat = v / (1 - beta2_pow)\n",
    "\n",
    "                d = self._get_slot(var, \"d\")\n",
    "\n",
    "                def first_step():\n",
    "                    op1 = tf.assign(self._f_2_hat, cost)\n",
    "                    op2 = tf.assign(d, tf.ones(d.shape))\n",
    "                    return tf.group(op1, op2)\n",
    "\n",
    "                def other_step():\n",
    "                    delte, Delte = tf.cond(\n",
    "                        tf.greater_equal(cost, self._f_2_hat),\n",
    "                        lambda: (tf.constant(self._k + 1.), tf.constant(self._K + 1.)),\n",
    "                        lambda: (tf.constant(1. / (self._K + 1.)), tf.constant(1. / (self._k + 1))))\n",
    "                    c_t = tf.minimum(tf.maximum(delte, cost / self._f_2_hat), Delte)\n",
    "                    f_1_hat = c_t * self._f_2_hat\n",
    "                    r_t = tf.abs(f_1_hat - self._f_2_hat) / tf.minimum(f_1_hat, self._f_2_hat)\n",
    "\n",
    "                    op1 = tf.assign(d, self._beta3 * d + (1 - self._beta3) * r_t)\n",
    "                    op2 = tf.assign(self._f_2_hat, f_1_hat)\n",
    "                    return tf.group(op1, op2)\n",
    "\n",
    "                update_algo_params = tf.cond(\n",
    "                    tf.greater(self._step, tf.constant(1, dtype=tf.int32)), other_step, first_step)\n",
    "                with tf.control_dependencies([update_algo_params]):\n",
    "                    update_param = var.assign_add(\n",
    "                        - self._lr * m_t_hat / (d * tf.sqrt(v_t_hat) + self._epsilon))\n",
    "\n",
    "                updates.append(update_param)\n",
    "\n",
    "            update_step = self._step.assign_add(1)\n",
    "            pr_step = tf.Print(update_step, [self._step], \"pritn step at the last\")\n",
    "            updates.append(update_step)\n",
    "            updates.append(pr_step)\n",
    "            return tf.group(*updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   --- Train loss: 729.305761   ---Train accuracy: 0.262097   ---Test loss: 4.289254   --- F: 0.408397\n",
      "Epoch 10   --- Train loss: 13.529293   ---Train accuracy: 0.989785   ---Test loss: 0.646005   --- F: 0.837990\n",
      "Epoch 20   --- Train loss: 1.040855   ---Train accuracy: 1.008065   ---Test loss: 0.545028   --- F: 0.862334\n",
      "Epoch 30   --- Train loss: 0.190975   ---Train accuracy: 1.008065   ---Test loss: 0.520026   --- F: 0.877986\n",
      "Epoch 40   --- Train loss: 0.039365   ---Train accuracy: 1.008065   ---Test loss: 0.507946   --- F: 0.882167\n",
      "Epoch 50   --- Train loss: 0.008091   ---Train accuracy: 1.008065   ---Test loss: 0.500289   --- F: 0.885496\n",
      "Epoch 60   --- Train loss: 0.002499   ---Train accuracy: 1.008065   ---Test loss: 0.507166   --- F: 0.891848\n",
      "Epoch 70   --- Train loss: 0.000824   ---Train accuracy: 1.008065   ---Test loss: 0.518447   --- F: 0.891984\n",
      "Epoch 80   --- Train loss: 0.000429   ---Train accuracy: 1.008065   ---Test loss: 0.530042   --- F: 0.895657\n",
      "Epoch 90   --- Train loss: 0.000223   ---Train accuracy: 1.008065   ---Test loss: 0.536781   --- F: 0.901222\n",
      "Epoch 100   --- Train loss: 0.000132   ---Train accuracy: 1.008065   ---Test loss: 0.546063   --- F: 0.900371\n",
      "Epoch 110   --- Train loss: 0.000093   ---Train accuracy: 1.008065   ---Test loss: 0.550244   --- F: 0.899602\n",
      "Epoch 120   --- Train loss: 0.000069   ---Train accuracy: 1.008065   ---Test loss: 0.555959   --- F: 0.901222\n",
      "Epoch 130   --- Train loss: 0.000051   ---Train accuracy: 1.008065   ---Test loss: 0.562314   --- F: 0.901130\n",
      "Epoch 140   --- Train loss: 0.000036   ---Train accuracy: 1.008065   ---Test loss: 0.569975   --- F: 0.902768\n",
      "Epoch 150   --- Train loss: 0.000026   ---Train accuracy: 1.008065   ---Test loss: 0.577900   --- F: 0.904417\n",
      "Epoch 160   --- Train loss: 0.000018   ---Train accuracy: 1.008065   ---Test loss: 0.587060   --- F: 0.901864\n",
      "Epoch 170   --- Train loss: 0.000012   ---Train accuracy: 1.008065   ---Test loss: 0.596638   --- F: 0.903411\n",
      "Epoch 180   --- Train loss: 0.000007   ---Train accuracy: 1.008065   ---Test loss: 0.608500   --- F: 0.904144\n",
      "Epoch 190   --- Train loss: 0.000004   ---Train accuracy: 1.008065   ---Test loss: 0.618093   --- F: 0.903484\n",
      "Elapsed time: 4.177262 min\n",
      "[7 5 8 9 0 6 6 3 9 5 8 2 0 6 9 0 8 0 1 3 2 0 2 5 7 2 1 1 9 0 5 7 7 1 3 3 2\n",
      " 5 8 7 9 8 6 9 6 2 4 6 3 4 7 5 1 0 9 2 1 2 0 4 5 3 7 2 1 1 5 8 6 4 4 0 9 8\n",
      " 4 4 0 9 8 2 9 2 3 8 7 4 0 2 6 2 2 7 7 1 2 7 2 2 3 0]\n",
      "[2 5 8 9 0 6 6 3 9 5 8 2 0 6 9 0 8 0 1 3 2 0 2 5 7 2 1 1 9 0 0 7 7 1 3 3 2\n",
      " 5 8 7 9 8 6 7 6 2 0 6 3 4 7 5 1 0 7 4 1 2 0 4 5 3 7 2 1 1 5 8 6 4 5 0 9 8\n",
      " 4 4 0 9 8 2 9 2 3 8 7 4 0 2 6 2 2 7 7 1 2 7 7 2 3 0]\n",
      "0.922939005439\n"
     ]
    }
   ],
   "source": [
    "# With Eve\n",
    "validate_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    IMAGE_SIZE = 784\n",
    "    CATEGORY_NUM = 10\n",
    "    ETA = 0.15\n",
    "    EPSILON = 1e-5\n",
    "    DECAY = 0.99\n",
    "\n",
    "    LAYER1_UNITS = 100\n",
    "    LAYER2_UNITS = 100\n",
    "    LAYER3_UNITS = CATEGORY_NUM\n",
    "\n",
    "    TRAIN_DATA_SIZE = len(train_X)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # todo(matthew): Add operation to calculate data ave and var before test evaluation\n",
    "\n",
    "    with tf.variable_scope('Placeholders'):\n",
    "        images = tf.placeholder(tf.float32, [None, IMAGE_SIZE], name='image_data')\n",
    "        labels = tf.placeholder(tf.int64, None, name='label')\n",
    "        _labels = tf.one_hot(labels, depth=CATEGORY_NUM, on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "        is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "    with tf.variable_scope('NetworkParams'):\n",
    "        W1 = tf.Variable(tf.truncated_normal([IMAGE_SIZE, LAYER1_UNITS]), name='W1')\n",
    "        W2 = tf.Variable(tf.truncated_normal([LAYER1_UNITS, LAYER2_UNITS]), name='W2')\n",
    "        W3 = tf.Variable(tf.truncated_normal([LAYER2_UNITS, LAYER3_UNITS]), name='W3')\n",
    "        b3 = tf.Variable(tf.zeros(LAYER3_UNITS), name='b3')\n",
    "        # params = [W1, W2, W3, b3]\n",
    "\n",
    "    def batch_normalization(X, is_train):\n",
    "        eps = EPSILON\n",
    "        output_dim = int(X.shape[-1])\n",
    "\n",
    "        gamma = tf.Variable(tf.truncated_normal([output_dim], stddev=0.1, mean=1.0), name='gamma')\n",
    "        beta = tf.Variable(tf.zeros([output_dim]), name='beta')\n",
    "\n",
    "        pop_mean = tf.Variable(tf.zeros([output_dim]), trainable=False)\n",
    "        pop_var = tf.Variable(tf.ones([output_dim]), trainable=False)\n",
    "\n",
    "        def sample():\n",
    "            mean_X, var_X = tf.nn.moments(X, [0])\n",
    "            # Save exponential average\n",
    "            moving_avg_op = tf.group(\n",
    "                pop_mean.assign(DECAY * pop_mean + (1 - DECAY) * mean_X),\n",
    "                pop_var.assign(DECAY * pop_var + (1 - DECAY) * var_X),\n",
    "            )\n",
    "\n",
    "            with tf.control_dependencies([moving_avg_op]):\n",
    "                batch_size = tf.to_float(tf.shape(X)[0])\n",
    "                unbiased_var_X = batch_size / (batch_size - 1) * var_X\n",
    "                return tf.nn.batch_normalization(X, mean_X, var_X, beta, gamma, eps)\n",
    "\n",
    "        def population():\n",
    "            return tf.nn.batch_normalization(X, pop_mean, pop_var, beta, gamma, eps)\n",
    "\n",
    "        return tf.cond(is_train, lambda: sample(), lambda: population())\n",
    "\n",
    "\n",
    "    u1 = batch_normalization(tf.matmul(images, W1), is_train)\n",
    "    z1 = tf.nn.relu(u1)\n",
    "    u2 = batch_normalization(tf.matmul(z1, W2), is_train)\n",
    "    z2 = tf.nn.relu(u2)\n",
    "    u3 = tf.matmul(z2, W3) + b3\n",
    "    y = u3\n",
    "    # y: [BATCH_SIZE x category_size]\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=_labels))\n",
    "    predicted = tf.argmax(y, axis=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), tf.float32))\n",
    "\n",
    "    # Update network params\n",
    "    params = tf.trainable_variables()\n",
    "#     grads = tf.gradients(cost, params)\n",
    "#     updates = [v.assign_add(- ETA * gv) for v, gv in zip(params, grads)]\n",
    "#     train = tf.group(*updates)\n",
    "    train = EveOptimizer().minimize(cost, params)\n",
    "#     train = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    # Prepare data\n",
    "    t_X, v_X, t_y, v_y = train_test_split(train_X, train_y)\n",
    "    \n",
    "    # Training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        train_summary_writer = tf.summary.FileWriter(\"./optimizer/train\", sess.graph)\n",
    "\n",
    "        EPOCHS = 200\n",
    "        BATCH_SIZE = 30\n",
    "        STEPS_IN_EPOCHS = len(t_X) // BATCH_SIZE\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Train\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            for step in range(STEPS_IN_EPOCHS):\n",
    "                start_idx = step * BATCH_SIZE\n",
    "                end_idx = start_idx + BATCH_SIZE\n",
    "\n",
    "                batch_X, batch_y = train_X[start_idx:end_idx], train_y[start_idx:end_idx]\n",
    "                _, c, a = sess.run([train, cost, accuracy],\n",
    "                                   feed_dict={images: t_X[start_idx:end_idx],\n",
    "                                              labels: t_y[start_idx:end_idx],\n",
    "                                              is_train: True})\n",
    "                epoch_loss += c\n",
    "                epoch_accuracy += a\n",
    "\n",
    "            c, p = sess.run([cost, predicted],\n",
    "                            feed_dict={images: v_X, labels: v_y, is_train: False})\n",
    "            if epoch % 10 == 0:\n",
    "                format_str = \"Epoch %d   --- Train loss: %f   ---Train accuracy: %f   ---Test loss: %f   --- F: %f\"\n",
    "                print(format_str % (epoch, epoch_loss, epoch_accuracy / step, c, f1_score(v_y, p, average='macro')))\n",
    "                \n",
    "        train_summary_writer.close()\n",
    "\n",
    "        # Evaluation\n",
    "        pred_y = sess.run(predicted, feed_dict={images: test_X, is_train: False})\n",
    "        elapsed_time = (time.time() - start_time) / 60  # min\n",
    "        print(\"Elapsed time: %f min\" % elapsed_time)\n",
    "        return pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "\n",
    "def _var_key(var):\n",
    "    return (var.op.graph, var.op.name)\n",
    "\n",
    "\n",
    "def _valid_dtypes():\n",
    "    # Valid types for loss, variables, and gradients\n",
    "    return set([tf.float16, tf.float32, tf.float64])\n",
    "\n",
    "\n",
    "def _assert_valid_dtypes(tensors):\n",
    "    valid_dtypes = _valid_dtypes()\n",
    "    for t in tensors:\n",
    "        dtype = t.dtype.base_dtype\n",
    "        if dtype not in valid_dtypes:\n",
    "            raise ValueError(\"Invalid type %r for %s, expected: %s\"\n",
    "                             % (dtype, t.name, [v for v in valid_dtypes]))\n",
    "\n",
    "\n",
    "class EveOptimizer:\n",
    "    def __init__(self, learning_rate=.001, beta1=0.9, beta2=0.999, beta3=0.999, epsilon=1e-8,\n",
    "                 k=.1, K=10, name=\"Eve\"):\n",
    "        self._lr = tf.constant(learning_rate, name='learning_rate')\n",
    "        self._beta1 = tf.constant(beta1, name='beta1')\n",
    "        self._beta2 = tf.constant(beta2, name='beta2')\n",
    "        self._beta3 = tf.constant(beta3, name='beta3')\n",
    "        self._epsilon = tf.constant(epsilon, name='epsilon')\n",
    "        self._k = k\n",
    "        self._K = K\n",
    "\n",
    "        self._beta1_power = None\n",
    "        self._beta2_power = None\n",
    "        self._f1_hat = None\n",
    "        self._step = None\n",
    "\n",
    "        # {slot1: {var1: xxx, var2: xxx, ...}, slot2: {var1: xxx, var2: xxx, ...}, ...}\n",
    "        self._slots = {}\n",
    "\n",
    "    def _get_beta_accumulators(self):\n",
    "        return self._beta1_power, self._beta2_power\n",
    "\n",
    "    def _accumulate_beta(self):\n",
    "        op1 = tf.assign(self._beta1_power, self._beta1_power * self._beta1)\n",
    "        op2 = tf.assign(self._beta2_power, self._beta2_power * self._beta2)\n",
    "        return tf.group(op1, op2)\n",
    "\n",
    "    def _create_algo_params(self, var_list):\n",
    "        if self._beta1_power is None or self._beta1_power.graph is not var_list[0].graph:\n",
    "            with ops.colocate_with(var_list[0]):\n",
    "                with tf.name_scope('Eve'):\n",
    "                    self._beta1_power = tf.Variable(1., name='beta1_power',\n",
    "                                                    trainable=False)\n",
    "                    self._beta2_power = tf.Variable(1., name='beta2_power',\n",
    "                                                    trainable=False)\n",
    "                    self._f1_hat = tf.Variable(0., name='f1_hat', trainable=False)\n",
    "                    self._step = tf.Variable(1, name='step', trainable=False)\n",
    "\n",
    "        for v in var_list:\n",
    "            with tf.name_scope('Eve'):\n",
    "                self._create_slot(v, tf.zeros(v.shape), \"m\")\n",
    "                self._create_slot(v, tf.zeros(v.shape), \"v\")\n",
    "                self._create_slot(v, tf.ones(v.shape), \"d\")\n",
    "\n",
    "    def _create_slot(self, var, initializer, slot_name):\n",
    "        \"\"\"\n",
    "        Create slots only in var's slot hasn't been registered.\n",
    "        \"\"\"\n",
    "        if slot_name not in self._slots.keys():\n",
    "            self._slots[slot_name] = {}\n",
    "        named_slot = self._slots[slot_name]\n",
    "\n",
    "        if _var_key(var) not in named_slot:\n",
    "            with tf.variable_scope(slot_name):\n",
    "                named_slot[_var_key(var)] = tf.Variable(initializer, name=var.op.name, trainable=False)\n",
    "\n",
    "    def _get_slot(self, var, slot_name):\n",
    "        named_slot = self._slots[slot_name]\n",
    "        return named_slot[_var_key(var)]\n",
    "\n",
    "    def _compute_gradients(self, cost, var_list=None):\n",
    "        _assert_valid_dtypes([cost])\n",
    "        if var_list is None:\n",
    "            var_list = tf.trainable_variables()\n",
    "        else:\n",
    "            nest.flatten(var_list)\n",
    "        grads = tf.gradients(cost, var_list)\n",
    "        grads_and_vars = list(zip(grads, var_list))\n",
    "        return grads_and_vars\n",
    "\n",
    "    def _apply_gradients(self, grads_and_vars, cost):\n",
    "        \"\"\"\n",
    "        grads_and_vars: [(g1, v1), (g2, v2), ...]\n",
    "        \"\"\"\n",
    "        var_list = [var for g, var in grads_and_vars]\n",
    "        self._create_algo_params(var_list)\n",
    "\n",
    "        beta_acc_op = self._accumulate_beta()\n",
    "        updates = []\n",
    "\n",
    "        with tf.control_dependencies([beta_acc_op]):\n",
    "            beta1_pow, beta2_pow = self._get_beta_accumulators()\n",
    "            for g, var in grads_and_vars:\n",
    "                m = self._get_slot(var, \"m\")\n",
    "\n",
    "                update_m = tf.assign(m, self._beta1 * m + (1 - self._beta1) * g)\n",
    "                with tf.control_dependencies([update_m]):\n",
    "                    updates.append(tf.assert_greater(1 - beta1_pow, 0.))\n",
    "                    m_t_hat = m / (1 - beta1_pow)\n",
    "\n",
    "                v = self._get_slot(var, \"v\")\n",
    "                update_v = tf.assign(v, self._beta2 * v + (1 - self._beta2) * g * g)\n",
    "                with tf.control_dependencies([update_v]):\n",
    "                    updates.append(tf.assert_greater(1 - beta2_pow, 0.))\n",
    "                    v_t_hat = v / (1 - beta2_pow)\n",
    "\n",
    "                def first_step():\n",
    "                    op1 = tf.assign(self._f1_hat, cost)\n",
    "                    op2 = tf.assign(d, tf.ones(d.shape))\n",
    "                    return tf.group(op1, op2)\n",
    "\n",
    "                def other_step():\n",
    "                    delte, Delte = tf.cond(\n",
    "                        tf.greater_equal(cost, self._f1_hat),\n",
    "                        lambda: (tf.constant(self._k + 1.), tf.constant(self._K + 1.)),\n",
    "                        lambda: (tf.constant(1. / (self._K + 1.)), tf.constant(1. / (self._k + 1))))\n",
    "                    c_t = tf.minimum(tf.maximum(delte, cost / self._f1_hat), Delte)\n",
    "                    f_1_hat = c_t * self._f1_hat\n",
    "                    r_t = tf.abs(f_1_hat - self._f1_hat) / tf.minimum(f_1_hat, self._f1_hat)\n",
    "\n",
    "                    op1 = tf.assign(d, self._beta3 * d + (1 - self._beta3) * r_t)\n",
    "                    op2 = tf.assign(self._f1_hat, f_1_hat)\n",
    "                    return tf.group(op1, op2)\n",
    "\n",
    "                d = self._get_slot(var, \"d\")\n",
    "                update_d = tf.cond(tf.greater(self._step, tf.constant(1, dtype=tf.int32)),\n",
    "                                   other_step, first_step)\n",
    "                with tf.control_dependencies([update_d]):\n",
    "                    update_param = var.assign_add(\n",
    "                        - self._lr * m_t_hat / (d * tf.sqrt(v_t_hat) + self._epsilon))\n",
    "                updates.append(update_param)\n",
    "\n",
    "            # Update self._step\n",
    "            update_step = self._step.assign_add(1)\n",
    "            updates.append(update_step)\n",
    "\n",
    "        return tf.group(*updates)\n",
    "\n",
    "    def minimize(self, cost, var_list=None):\n",
    "        grads_and_vars = self._compute_gradients(cost, var_list)\n",
    "        updates = self._apply_gradients(grads_and_vars, cost)\n",
    "        return updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   --- Train loss: 688.653606   ---Train accuracy: 0.276882   ---Test loss: 4.359731   --- F: 0.416045\n",
      "Epoch 10   --- Train loss: 16.627619   ---Train accuracy: 0.981452   ---Test loss: 0.568213   --- F: 0.861912\n",
      "Epoch 20   --- Train loss: 1.548400   ---Train accuracy: 1.008065   ---Test loss: 0.530950   --- F: 0.882425\n",
      "Epoch 30   --- Train loss: 0.361680   ---Train accuracy: 1.008065   ---Test loss: 0.541090   --- F: 0.891300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-44345d7883f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidate_homework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-950667387dfc>\u001b[0m in \u001b[0;36mvalidate_homework\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtest_y_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhomework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fcfbe6b81046>\u001b[0m in \u001b[0;36mhomework\u001b[0;34m(train_X, train_y, test_X)\u001b[0m\n\u001b[1;32m    106\u001b[0m                                    feed_dict={images: t_X[start_idx:end_idx],\n\u001b[1;32m    107\u001b[0m                                               \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                                               is_train: True})\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mepoch_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luning/.pyenv/versions/3.6.0/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luning/.pyenv/versions/3.6.0/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luning/.pyenv/versions/3.6.0/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/luning/.pyenv/versions/3.6.0/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luning/.pyenv/versions/3.6.0/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validate_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
