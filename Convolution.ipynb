{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "mnist_X, mnist_y = mnist.train.images, mnist.train.labels\n",
    "mnist_X = mnist_X.reshape((mnist_X.shape[0], 28, 28, 1))\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(mnist_X, mnist_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 入力 (4次元)\n",
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "# サンプル画像\n",
    "sample_image = np.array([[1, 1, 1, 0, 0],\n",
    "                         [0, 1, 1, 1, 0],\n",
    "                         [0, 0, 1, 1, 1],\n",
    "                         [0, 0, 1, 1, 0],\n",
    "                         [0, 1, 1, 0, 0]]\n",
    "                       ).astype('float32').reshape(1, 5, 5, 1)\n",
    "\n",
    "# フィルタ\n",
    "W = np.array([[1, 0, 1],\n",
    "              [0, 1, 0],\n",
    "              [1, 0, 1]]).astype('float32').reshape(3, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1) [[[[ 4.]\n",
      "   [ 3.]\n",
      "   [ 4.]]\n",
      "\n",
      "  [[ 2.]\n",
      "   [ 4.]\n",
      "   [ 3.]]\n",
      "\n",
      "  [[ 2.]\n",
      "   [ 3.]\n",
      "   [ 4.]]]]\n"
     ]
    }
   ],
   "source": [
    "convoluted_image = tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    res = sess.run(convoluted_image, feed_dict={x: sample_image})\n",
    "    print(res.shape, res)\n",
    "#     print(sess.run(convoluted_image, feed_dict={x: sample_image}).reshape(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(weight_1[i].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "del [\n",
    "    tf.app,\n",
    "    tf.compat,\n",
    "    tf.contrib,\n",
    "    tf.errors,\n",
    "    tf.gfile,\n",
    "    tf.graph_util,\n",
    "    tf.image,\n",
    "    tf.layers,\n",
    "    tf.logging,\n",
    "    tf.losses,\n",
    "    tf.metrics,\n",
    "    tf.python_io,\n",
    "    tf.resource_loader,\n",
    "    tf.saved_model,\n",
    "    tf.sdca,\n",
    "    tf.sets,\n",
    "    tf.summary,\n",
    "    tf.sysconfig,\n",
    "    tf.test\n",
    "]\n",
    "\n",
    "def load_mnist():\n",
    "    mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "    mnist_X = np.r_[mnist.train.images, mnist.test.images]\n",
    "    mnist_y = np.r_[mnist.train.labels, mnist.test.labels]\n",
    "    return train_test_split(mnist_X, mnist_y, test_size=0.2, random_state=42)\n",
    "\n",
    "def validate_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "    train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 28, 28, 1))\n",
    "\n",
    "    # validate for small dataset\n",
    "    train_X_mini = train_X[:100]\n",
    "    train_y_mini = train_y[:100]\n",
    "    test_X_mini = test_X[:100]\n",
    "    test_y_mini = test_y[:100]\n",
    "\n",
    "    pred_y = homework(train_X_mini, train_y_mini, test_X_mini)\n",
    "    print(f1_score(np.argmax(test_y_mini, 1), pred_y, average='macro'))\n",
    "\n",
    "def score_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "    train_X = train_X.reshape((train_X.shape[0], 28, 28, 1))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 28, 28, 1))\n",
    "    \n",
    "    pred_y = homework(train_X, train_y, test_X)\n",
    "    print(f1_score(np.argmax(test_y, 1), pred_y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    t_X, v_X, t_y, v_y = train_test_split(train_X, train_y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1], name='X')\n",
    "    t = tf.placeholder(tf.float32, [None, 10], name='t')\n",
    "    \n",
    "    with tf.variable_scope('filters'):\n",
    "        f1 = tf.Variable(tf.truncated_normal([10, 10, 1, 5]), name='f1')\n",
    "        b1 = tf.Variable(tf.zeros(5), name='b1')\n",
    "        f2 = tf.Variable(tf.truncated_normal([5, 5, 5, 8]), name='f2')\n",
    "        b2 = tf.Variable(tf.zeros(8), name='b2')\n",
    "\n",
    "        f3 = tf.Variable(tf.truncated_normal([10, 10, 8, 8]), name='f1')\n",
    "        b3 = tf.Variable(tf.zeros(8), name='b1')\n",
    "        f4= tf.Variable(tf.truncated_normal([5, 5, 8, 8]), name='f2')\n",
    "        b4 = tf.Variable(tf.zeros(8), name='b2')\n",
    "\n",
    "        W1 = tf.Variable(tf.truncated_normal([28 * 28 * 8, 10]), name='W1')\n",
    "        b5 = tf.Variable(tf.zeros(10), name='b5')\n",
    "        \n",
    "    # Convolution 1\n",
    "    u1 = tf.nn.conv2d(X, f1, strides=[1,1,1,1], padding='SAME') + b1\n",
    "    z1 = tf.nn.relu(u1)\n",
    "    # Convolution 2\n",
    "    u2 = tf.nn.conv2d(z1, f2, strides=[1,1,1,1], padding='SAME') + b2\n",
    "    z2 = tf.nn.relu(u2)\n",
    "    \n",
    "    # Max Pooling\n",
    "    p1 = tf.nn.max_pool(z2, ksize=[1, 4, 4, 1], strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    # Convolution 3\n",
    "    u3 = tf.nn.conv2d(p1, f3, strides=[1,1,1,1], padding='SAME') + b3\n",
    "    z3 = tf.nn.relu(u3)\n",
    "    # Convolution 4\n",
    "    u4 = tf.nn.conv2d(z3, f3, strides=[1,1,1,1], padding='SAME') + b4\n",
    "    z4 = tf.nn.relu(u4)\n",
    "    \n",
    "    # Max Pooling\n",
    "    p2 = tf.nn.max_pool(z4, ksize=[1, 4, 4, 1], strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    # FC layer\n",
    "#     print(p2)\n",
    "    flattened = tf.reshape(p2, [-1, np.prod(p2.shape.as_list()[1:])])\n",
    "    u5 = tf.matmul(flattened, W1) + b5\n",
    "    predicted = tf.argmax(u5, axis=1)\n",
    "    \n",
    "    cost = tf.nn.softmax_cross_entropy_with_logits(logits=u5, labels=t)\n",
    "    train = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    # Train\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        epochs = 10\n",
    "        batch_size = 10\n",
    "        steps_in_epoch = t_X.shape[0] // batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for step in range(steps_in_epoch):\n",
    "                start_idx = step * batch_size\n",
    "                end_idx = (step + 1) * batch_size\n",
    "                _, c = sess.run([train, cost],\n",
    "                                           feed_dict={X: t_X[start_idx:end_idx], t: t_y[start_idx:end_idx]})\n",
    "            c, p = sess.run(cost, predicted, feed_dict={X: v_X, t: v_y})\n",
    "            format_str = \"Epoch: %d   ---Valid cost %f   ---Valid f %f\"\n",
    "            print(format_str % (epoch, c, f_score(np.argmax(v_y, 1).astype('int32'),\n",
    "                                                                               p, average='macro')))\n",
    "        pred_y = sess.run(predicted, feed_dict={X: test_X})\n",
    "        \n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    t_X, v_X, t_y, v_y = train_test_split(train_X, train_y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1], name='X')\n",
    "    t = tf.placeholder(tf.float32, [None, 10], name='t')\n",
    "    \n",
    "    with tf.variable_scope('filters'):\n",
    "        f1 = tf.Variable(tf.truncated_normal([5, 5, 1, 1]), name='f1')\n",
    "        b1 = tf.Variable(tf.zeros(1), name='b1')\n",
    "        f2 = tf.Variable(tf.truncated_normal([5, 5, 1, 1]), name='f2')\n",
    "        b2 = tf.Variable(tf.zeros(1), name='b2')\n",
    "\n",
    "        f3 = tf.Variable(tf.truncated_normal([5, 5, 1, 1]), name='f1')\n",
    "        b3 = tf.Variable(tf.zeros(1), name='b1')\n",
    "#         f4= tf.Variable(tf.truncated_normal([5, 5, 1, 1]), name='f2')\n",
    "#         b4 = tf.Variable(tf.zeros(1), name='b2')\n",
    "\n",
    "        W1 = tf.Variable(tf.truncated_normal([28 * 28 * 1, 10]), name='W1')\n",
    "        b5 = tf.Variable(tf.zeros(10), name='b5')\n",
    "        \n",
    "    # Convolution 1\n",
    "    u1 = tf.nn.conv2d(X, f1, strides=[1,1,1,1], padding='SAME') + b1\n",
    "    z1 = tf.nn.relu(u1)\n",
    "    # Convolution 2\n",
    "    u2 = tf.nn.conv2d(z1, f2, strides=[1,1,1,1], padding='SAME') + b2\n",
    "    z2 = tf.nn.relu(u2)\n",
    "    \n",
    "    # Max Pooling\n",
    "    p1 = tf.nn.max_pool(z2, ksize=[1, 4, 4, 1], strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    # Convolution 3\n",
    "    u3 = tf.nn.conv2d(p1, f3, strides=[1,1,1,1], padding='SAME') + b3\n",
    "    z3 = tf.nn.relu(u3)\n",
    "    # Convolution 4\n",
    "#     u4 = tf.nn.conv2d(z3, f3, strides=[1,1,1,1], padding='SAME') + b4\n",
    "#     z4 = tf.nn.relu(u4)\n",
    "    \n",
    "    # Max Pooling\n",
    "    p2 = tf.nn.max_pool(z3, ksize=[1, 8, 8, 1], strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    # FC layer\n",
    "    flattened = tf.reshape(p2, [-1, np.prod(p2.shape.as_list()[1:])])\n",
    "    u5 = tf.matmul(flattened, W1) + b5\n",
    "    predicted = tf.argmax(u5, axis=1)\n",
    "    \n",
    "    cost = tf.nn.softmax_cross_entropy_with_logits(logits=u5, labels=t)\n",
    "    train = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    # Train\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        epochs = 10\n",
    "        batch_size = 10\n",
    "        steps_in_epoch = t_X.shape[0] // batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for step in range(steps_in_epoch):\n",
    "                start_idx = step * batch_size\n",
    "                end_idx = (step + 1) * batch_size\n",
    "                _, c = sess.run([train, cost],\n",
    "                                           feed_dict={X: t_X[start_idx:end_idx], t: t_y[start_idx:end_idx]})\n",
    "            c, p = sess.run(cost, predicted, feed_dict={X: v_X, t: v_y})\n",
    "            format_str = \"Epoch: %d   ---Valid cost %f   ---Valid f %f\"\n",
    "            print(format_str % (epoch, c, f_score(np.argmax(v_y, 1).astype('int32'),\n",
    "                                                                               p, average='macro')))\n",
    "        pred_y = sess.run(predicted, feed_dict={X: test_X})\n",
    "        \n",
    "    return pred_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
